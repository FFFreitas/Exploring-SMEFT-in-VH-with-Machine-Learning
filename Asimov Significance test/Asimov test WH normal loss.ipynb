{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First load os and sys so I can update the sys.path with new functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#take the paths to the functions we nedd\n",
    "module_path = os.path.abspath(os.path.join('./pandasPlotting/'))\n",
    "module2_path = os.path.abspath(os.path.join('./MlClasses/'))\n",
    "module3_path = os.path.abspath(os.path.join('./MlFunctions/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this part will include in the sys.path variables the paths for our new functions\n",
    "if [module_path, module2_path, module3_path] not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# here we are going to load what we will need, keras + tensorflow, plot functions, etc..\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "\n",
    "from keras import callbacks\n",
    "\n",
    "from pandasPlotting.Plotter import Plotter\n",
    "from pandasPlotting.dfFunctions import expandArrays\n",
    "from pandasPlotting.dtFunctions import featureImportance\n",
    "\n",
    "from MlClasses.MlData import MlData\n",
    "from MlClasses.Bdt import Bdt\n",
    "from MlClasses.Dnn import Dnn\n",
    "from MlClasses.ComparePerformances import ComparePerformances\n",
    "\n",
    "from MlFunctions.DnnFunctions import significanceLoss,significanceLossInvert,significanceLoss2Invert ,significanceLossInvertSqrt,significanceFull,asimovSignificanceLoss,asimovSignificanceLossInvert,asimovSignificanceFull,truePositive,falsePositive\n",
    "\n",
    "from linearAlgebraFunctions import gram,addGramToFlatDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to scale our data\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "def scaleColumns(df, cols_to_scale):\n",
    "    for col in cols_to_scale:\n",
    "        df[col] = pd.DataFrame(min_max_scaler.fit_transform(pd.DataFrame(df[col])),columns=[col])\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I don't have patience to training 200 epochs ¯\\_(ツ)_/¯\n",
    "earlyStopping = callbacks.EarlyStopping(monitor='val_loss',min_delta=0,patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load our data files\n",
    "signal=pd.read_csv(\"../pyROOT_CPV_CPC/pp_wh/2dplots/analysis_with_cut/cpv_scan/charanjit_data/1L/inclusive/wh1.csv\",sep='\\s+',engine='python')\n",
    "bkgd=pd.read_csv(\"../pyROOT_CPV_CPC/pp_wh/2dplots/analysis_with_cut/cpv_scan/charanjit_data/1L/inclusive/whsm.csv\",sep='\\s+',engine='python')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        1\n",
       "1        1\n",
       "2        1\n",
       "3        1\n",
       "4        1\n",
       "5        1\n",
       "6        1\n",
       "7        1\n",
       "8        1\n",
       "9        1\n",
       "10       1\n",
       "11       1\n",
       "12       1\n",
       "13       1\n",
       "14       1\n",
       "15       1\n",
       "16       1\n",
       "17       1\n",
       "18       1\n",
       "19       1\n",
       "20       1\n",
       "21       1\n",
       "22       1\n",
       "23       1\n",
       "24       1\n",
       "25       1\n",
       "26       1\n",
       "27       1\n",
       "28       1\n",
       "29       1\n",
       "        ..\n",
       "99970    1\n",
       "99971    1\n",
       "99972    1\n",
       "99973    1\n",
       "99974    1\n",
       "99975    1\n",
       "99976    1\n",
       "99977    1\n",
       "99978    1\n",
       "99979    1\n",
       "99980    1\n",
       "99981    1\n",
       "99982    1\n",
       "99983    1\n",
       "99984    1\n",
       "99985    1\n",
       "99986    1\n",
       "99987    1\n",
       "99988    1\n",
       "99989    1\n",
       "99990    1\n",
       "99991    1\n",
       "99992    1\n",
       "99993    1\n",
       "99994    1\n",
       "99995    1\n",
       "99996    1\n",
       "99997    1\n",
       "99998    1\n",
       "99999    1\n",
       "Name: signal, Length: 100000, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "signal['signal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine them into one dataset\n",
    "combined = pd.concat([signal,bkgd]).sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['ptb1', 'ptb2', 'ptl', 'missinget', 'pth', 'ptw', 'etah', 'phih',\n",
      "       'deltarwl', 'mtW', 'mtvh', 'ptvh', 'delphibl', 'delphilmet', 'signal'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(combined.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54588    1\n",
       "92181    1\n",
       "1204     0\n",
       "84475    0\n",
       "1560     1\n",
       "13689    0\n",
       "27221    1\n",
       "37995    0\n",
       "55958    0\n",
       "89612    1\n",
       "67722    1\n",
       "8311     0\n",
       "14670    1\n",
       "31640    0\n",
       "92703    0\n",
       "89210    1\n",
       "70448    1\n",
       "64882    1\n",
       "94444    1\n",
       "12465    1\n",
       "51410    0\n",
       "22964    0\n",
       "52603    0\n",
       "13246    1\n",
       "51597    0\n",
       "88703    1\n",
       "48154    1\n",
       "22605    1\n",
       "36071    0\n",
       "15249    0\n",
       "        ..\n",
       "155      1\n",
       "63969    0\n",
       "98854    0\n",
       "52181    0\n",
       "72396    1\n",
       "88108    1\n",
       "8605     0\n",
       "56775    0\n",
       "6066     1\n",
       "40942    0\n",
       "10111    1\n",
       "78406    1\n",
       "98484    0\n",
       "48623    1\n",
       "9030     0\n",
       "10905    1\n",
       "8852     0\n",
       "21496    1\n",
       "82780    0\n",
       "7178     0\n",
       "92051    0\n",
       "22327    1\n",
       "43616    0\n",
       "45306    1\n",
       "65502    1\n",
       "82186    0\n",
       "47642    0\n",
       "94807    0\n",
       "41795    1\n",
       "24219    1\n",
       "Name: signal, Length: 200000, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined['signal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change thes vars depend on which dataset you are loading, I will implement a better solution.\n",
    "chosenVars = {\n",
    "            '1L':['ptb1', 'ptb2', 'ptl', 'missinget', 'pth', 'ptw', 'etah', 'phih',\n",
    "                  'deltarwl', 'mtW', 'mtvh', 'ptvh', 'delphibl', 'delphilmet', 'signal']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainedModels={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#needed to plot asimov significane\n",
    "asimovSigLossSysts=[0.01,0.05,0.1,0.2,0.3,0.4,0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here I have included one archtecture I got from my ES scan, pls comment my entry and use dnn_batch4096 instead.\n",
    "dnnConfigs={\n",
    "    'dnn_WH_1L_cHW_0d03_batch_128':{'epochs':200,'batch_size':128,'dropOut':0.2,'l2Regularization':None,'hiddenLayers':[128],\n",
    "                 'optimizer':'rmsprop', 'activation':'tanh'},\n",
    "    'dnn_WH_1L_cHW_0d03_batch_256':{'epochs':200,'batch_size':256,'dropOut':0.2,'l2Regularization':None,'hiddenLayers':[128],\n",
    "                 'optimizer':'rmsprop', 'activation':'tanh'},\n",
    "    'dnn_WH_1L_cHW_0d03_batch_512':{'epochs':200,'batch_size':512,'dropOut':0.2,'l2Regularization':None,'hiddenLayers':[128],\n",
    "                 'optimizer':'rmsprop', 'activation':'tanh'},\n",
    "    'dnn_WH_1L_cHW_0d03_batch_1024':{'epochs':200,'batch_size':1024,'dropOut':0.2,'l2Regularization':None,'hiddenLayers':[128],\n",
    "                 'optimizer':'rmsprop', 'activation':'tanh'}    \n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lumi=30. #luminosity in /fb\n",
    "expectedSignal=500*lumi \n",
    "expectedBkgd=200*lumi #cross section of ttbar sample in fb times efficiency measured by Marco\n",
    "systematic=0.1 #systematic for the asimov signficance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'm only running the DNN with binary cross entropy, letter I will include the other options.\n",
    "# so far it running perfect, I'm testing with their custom loss function.\n",
    "for varSetName,varSet in chosenVars.items():\n",
    "    #Pick out the expanded arrays\n",
    "    columnsInDataFrame = []\n",
    "    for k in combined.keys():\n",
    "        for v in varSet:\n",
    "            #Little trick to ensure only the start of the string is checked\n",
    "            if varSetName is '1L':\n",
    "                if ' '+v+' ' in ' '+k+' ': columnsInDataFrame.append(k)\n",
    "            elif ' '+v in ' '+k: columnsInDataFrame.append(k)\n",
    "\n",
    "\n",
    "    #Select just the features we're interested in\n",
    "    #For now setting NaNs to 0 for compatibility\n",
    "    combinedToRun = combined[columnsInDataFrame].copy()\n",
    "    combinedToRun.fillna(0,inplace=True)\n",
    "    \n",
    "    combinedToRun.index = np.arange(0,200000)\n",
    "    mlData = MlData(combinedToRun,'signal')\n",
    "\n",
    "    mlData.prepare(evalSize=0.0,testSize=0.3,limitSize=None)\n",
    "\n",
    "    for name,config in dnnConfigs.items():\n",
    "        dnn = Dnn(mlData,'testPlots/mlPlots/'+varSetName+'/'+name)\n",
    "        dnn.setup(hiddenLayers=config['hiddenLayers'], dropOut=config['dropOut'],\n",
    "                  l2Regularization=config['l2Regularization'], optimizer=config['optimizer'],\n",
    "                  activation=config['activation'],\n",
    "                extraMetrics=[\n",
    "                    significanceLoss(expectedSignal,expectedBkgd),significanceFull(expectedSignal,expectedBkgd),\n",
    "                    asimovSignificanceFull(expectedSignal,expectedBkgd,systematic),truePositive,falsePositive\n",
    "                    ])\n",
    "        dnn.fit(epochs=config['epochs'],batch_size=config['batch_size'],callbacks=[earlyStopping])\n",
    "        \n",
    "        dnn.explainPredictions()\n",
    "        dnn.diagnostics(batchSize=config['batch_size'])\n",
    "        dnn.makeHepPlots(expectedSignal,expectedBkgd,asimovSigLossSysts,makeHistograms=False)\n",
    "\n",
    "        trainedModels[varSetName+'_'+name]=dnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow-GPU",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
