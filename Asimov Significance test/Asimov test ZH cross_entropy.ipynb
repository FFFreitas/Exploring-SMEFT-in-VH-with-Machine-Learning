{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First load os and sys so I can update the sys.path with new functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "change the luminosity to 80 /fb\n",
    "\n",
    "generate the 3 plots as in aewol paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#take the paths to the functions we nedd\n",
    "module_path = os.path.abspath(os.path.join('./pandasPlotting/'))\n",
    "module2_path = os.path.abspath(os.path.join('./MlClasses/'))\n",
    "module3_path = os.path.abspath(os.path.join('./MlFunctions/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this part will include in the sys.path variables the paths for our new functions\n",
    "if [module_path, module2_path, module3_path] not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# here we are going to load what we will need, keras + tensorflow, plot functions, etc..\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "\n",
    "from keras import callbacks\n",
    "\n",
    "from pandasPlotting.Plotter import Plotter\n",
    "from pandasPlotting.dfFunctions import expandArrays\n",
    "from pandasPlotting.dtFunctions import featureImportance\n",
    "\n",
    "from MlClasses.MlData import MlData\n",
    "from MlClasses.Bdt import Bdt\n",
    "from MlClasses.Dnn import Dnn\n",
    "from MlClasses.ComparePerformances import ComparePerformances\n",
    "\n",
    "from MlFunctions.DnnFunctions import significanceLoss,significanceLossInvert,significanceLoss2Invert ,significanceLossInvertSqrt,significanceFull,asimovSignificanceLoss,asimovSignificanceLossInvert,asimovSignificanceFull,truePositive,falsePositive\n",
    "\n",
    "from linearAlgebraFunctions import gram,addGramToFlatDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I don't have patience to training 200 epochs ¯\\_(ツ)_/¯\n",
    "earlyStopping = callbacks.EarlyStopping(monitor='val_loss',min_delta=0,patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load our data files\n",
    "signal=pd.read_csv(\"../pyROOT_CPV_CPC/pp_wh/2dplots/analysis_with_cut/cpv_scan/charanjit_data/0L/feb/feb-0l-BSMlike/zh0p030lTCfeb.csv\",sep='\\s+',engine='python')\n",
    "bkgd=pd.read_csv(\"../pyROOT_CPV_CPC/pp_wh/2dplots/analysis_with_cut/cpv_scan/charanjit_data/0L/feb/feb-0l-BSMlike/zhsm0lTCfeb.csv\",sep='\\s+',engine='python')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine them into one dataset\n",
    "combined = pd.concat([signal,bkgd]).sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['ptb1', 'ptb2', 'misset', 'pth', 'ptz', 'etah', 'phih', 'mtvh', 'ptvh',\n",
      "       'dphib1met', 'signal'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(combined.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change thes vars depend on which dataset you are loading, I will implement a better solution.\n",
    "chosenVars = {\n",
    "            # #A vanilla analysis with HL variables and lead 3 jets\n",
    "            '0L':['ptb1', 'ptb2', 'misset', 'pth', 'ptz', 'etah', 'phih', 'mtvh', 'ptvh', 'dphib1met', 'signal'],\n",
    "            #'2L':['ptb1', 'ptb2', 'ptl1', 'ptl2', 'pth', 'ptz', 'etah', 'phih',\n",
    "            #     'deltarll', 'deltarbl', 'mtvh', 'ptvh', 'dphil1b1', 'dphil1b2', 'signal']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainedModels={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#needed to plot asimov significane\n",
    "asimovSigLossSysts=[0.01,0.05,0.1,0.2,0.3,0.4,0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here I have included one archtecture I got from my ES scan, pls comment my entry and use dnn_batch4096 instead.\n",
    "dnnConfigs={\n",
    "    #'dnn_ZH_0L_cHW_0d001_batch_1024':{'epochs':200,'batch_size':1024,'dropOut':0.2,'l2Regularization':None,'hiddenLayers':[1.0],\n",
    "    #             'optimizer':'adam', 'activation':'relu'}\n",
    "    #'dnn_ZH_0L_cHW_0d01_batch_1024':{'epochs':200,'batch_size':1024,'dropOut':0.2,'l2Regularization':None,'hiddenLayers':[1.0],\n",
    "    #             'optimizer':'adam', 'activation':'relu'}\n",
    "    'dnn_ZH_0L_cHW_0d03_batch_1024':{'epochs':200,'batch_size':1024,'dropOut':0.2,'l2Regularization':None,'hiddenLayers':[1.0],\n",
    "                 'optimizer':'adam', 'activation':'relu'}\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bsmlike signal cHW 0.03: 11.867fb, bg:0.89\n",
    "#inclusive signal cHW 0.001: 2.198fb, bg:2.03fb\n",
    "#inclusive signal cHW 0.01: 4.553fb, bg:2.03fb\n",
    "#inclusive signal cHW 0.03: 14fb, bg:2.03fb\n",
    "\n",
    "lumi=80. #luminosity in /fb\n",
    "expectedSignal=11.867*lumi \n",
    "expectedBkgd=0.89*lumi #cross section of ttbar sample in fb times efficiency measured by Marco\n",
    "systematic=0.5 #systematic for the asimov signficance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/felipe/hepML/MlClasses/Dnn.py:101: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  self.history = self.model.fit(self.data.X_train.as_matrix(), self.data.y_train.as_matrix(), sample_weight=self.data.weights_train,\n",
      "/home/felipe/hepML/MlClasses/Dnn.py:102: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  validation_data=(self.data.X_test.as_matrix(),self.data.y_test.as_matrix(),self.data.weights_test),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 140000 samples, validate on 60000 samples\n",
      "Epoch 1/200\n",
      "140000/140000 [==============================] - 1s 8us/step - loss: 0.6283 - acc: 0.6324 - sigLoss: -489.1391 - significance: 22.0866 - asimovSignificance: 13.4381 - truePositive: 0.5380 - falsePositive: 0.2731 - val_loss: 0.5475 - val_acc: 0.7240 - val_sigLoss: -545.8971 - val_significance: 23.2534 - val_asimovSignificance: 19.0567 - val_truePositive: 0.5794 - val_falsePositive: 0.1315\n",
      "Epoch 2/200\n",
      "140000/140000 [==============================] - 0s 3us/step - loss: 0.5495 - acc: 0.7168 - sigLoss: -560.3888 - significance: 23.6934 - asimovSignificance: 17.2776 - truePositive: 0.6040 - falsePositive: 0.1705 - val_loss: 0.5360 - val_acc: 0.7293 - val_sigLoss: -573.0522 - val_significance: 23.7257 - val_asimovSignificance: 18.6202 - val_truePositive: 0.6038 - val_falsePositive: 0.1453\n",
      "Epoch 3/200\n",
      "140000/140000 [==============================] - 0s 3us/step - loss: 0.5435 - acc: 0.7241 - sigLoss: -575.0624 - significance: 23.9266 - asimovSignificance: 17.6065 - truePositive: 0.6156 - falsePositive: 0.1674 - val_loss: 0.5349 - val_acc: 0.7302 - val_sigLoss: -577.2393 - val_significance: 23.7765 - val_asimovSignificance: 18.6018 - val_truePositive: 0.6064 - val_falsePositive: 0.1463\n",
      "Epoch 4/200\n",
      "140000/140000 [==============================] - 0s 3us/step - loss: 0.5415 - acc: 0.7268 - sigLoss: -577.2522 - significance: 23.9620 - asimovSignificance: 17.8188 - truePositive: 0.6170 - falsePositive: 0.1635 - val_loss: 0.5344 - val_acc: 0.7310 - val_sigLoss: -576.4155 - val_significance: 23.8036 - val_asimovSignificance: 18.6455 - val_truePositive: 0.6078 - val_falsePositive: 0.1458\n",
      "Epoch 5/200\n",
      "140000/140000 [==============================] - 0s 3us/step - loss: 0.5401 - acc: 0.7271 - sigLoss: -579.0347 - significance: 23.9807 - asimovSignificance: 17.8188 - truePositive: 0.6179 - falsePositive: 0.1637 - val_loss: 0.5341 - val_acc: 0.7308 - val_sigLoss: -575.9639 - val_significance: 23.8331 - val_asimovSignificance: 18.5436 - val_truePositive: 0.6094 - val_falsePositive: 0.1480\n",
      "Epoch 6/200\n",
      "140000/140000 [==============================] - 0s 2us/step - loss: 0.5386 - acc: 0.7304 - sigLoss: -578.0377 - significance: 24.0556 - asimovSignificance: 18.0391 - truePositive: 0.6215 - falsePositive: 0.1607 - val_loss: 0.5339 - val_acc: 0.7317 - val_sigLoss: -580.8015 - val_significance: 24.0126 - val_asimovSignificance: 18.2635 - val_truePositive: 0.6190 - val_falsePositive: 0.1558\n",
      "Epoch 7/200\n",
      "140000/140000 [==============================] - 0s 3us/step - loss: 0.5378 - acc: 0.7300 - sigLoss: -579.7273 - significance: 24.1050 - asimovSignificance: 17.8919 - truePositive: 0.6243 - falsePositive: 0.1642 - val_loss: 0.5336 - val_acc: 0.7318 - val_sigLoss: -578.3174 - val_significance: 23.9913 - val_asimovSignificance: 18.3162 - val_truePositive: 0.6178 - val_falsePositive: 0.1544\n",
      "Epoch 8/200\n",
      "140000/140000 [==============================] - 0s 3us/step - loss: 0.5376 - acc: 0.7316 - sigLoss: -580.2027 - significance: 24.1495 - asimovSignificance: 17.9824 - truePositive: 0.6265 - falsePositive: 0.1632 - val_loss: 0.5336 - val_acc: 0.7319 - val_sigLoss: -579.1530 - val_significance: 24.0770 - val_asimovSignificance: 18.1561 - val_truePositive: 0.6225 - val_falsePositive: 0.1589\n",
      "Epoch 9/200\n",
      "140000/140000 [==============================] - 0s 3us/step - loss: 0.5369 - acc: 0.7319 - sigLoss: -579.7705 - significance: 24.1842 - asimovSignificance: 17.9568 - truePositive: 0.6283 - falsePositive: 0.1645 - val_loss: 0.5333 - val_acc: 0.7318 - val_sigLoss: -577.5114 - val_significance: 24.0434 - val_asimovSignificance: 18.2193 - val_truePositive: 0.6207 - val_falsePositive: 0.1572\n",
      "Epoch 10/200\n",
      "140000/140000 [==============================] - 0s 3us/step - loss: 0.5364 - acc: 0.7326 - sigLoss: -580.2800 - significance: 24.2285 - asimovSignificance: 17.9557 - truePositive: 0.6307 - falsePositive: 0.1653 - val_loss: 0.5332 - val_acc: 0.7319 - val_sigLoss: -578.8084 - val_significance: 24.1243 - val_asimovSignificance: 18.0557 - val_truePositive: 0.6251 - val_falsePositive: 0.1615\n",
      "Epoch 11/200\n",
      "140000/140000 [==============================] - 0s 2us/step - loss: 0.5361 - acc: 0.7320 - sigLoss: -580.2263 - significance: 24.2567 - asimovSignificance: 17.8109 - truePositive: 0.6324 - falsePositive: 0.1683 - val_loss: 0.5331 - val_acc: 0.7317 - val_sigLoss: -581.8453 - val_significance: 24.2205 - val_asimovSignificance: 17.8520 - val_truePositive: 0.6304 - val_falsePositive: 0.1671\n",
      "Epoch 12/200\n",
      "140000/140000 [==============================] - 0s 2us/step - loss: 0.5361 - acc: 0.7325 - sigLoss: -581.2051 - significance: 24.2921 - asimovSignificance: 17.8056 - truePositive: 0.6342 - falsePositive: 0.1690 - val_loss: 0.5330 - val_acc: 0.7318 - val_sigLoss: -578.3753 - val_significance: 24.1699 - val_asimovSignificance: 17.9630 - val_truePositive: 0.6276 - val_falsePositive: 0.1640\n",
      "Epoch 13/200\n",
      "140000/140000 [==============================] - 0s 3us/step - loss: 0.5352 - acc: 0.7328 - sigLoss: -580.8755 - significance: 24.2978 - asimovSignificance: 17.8199 - truePositive: 0.6345 - falsePositive: 0.1688 - val_loss: 0.5330 - val_acc: 0.7319 - val_sigLoss: -579.0921 - val_significance: 24.2267 - val_asimovSignificance: 17.8571 - val_truePositive: 0.6307 - val_falsePositive: 0.1670\n",
      "Epoch 14/200\n",
      "140000/140000 [==============================] - 0s 3us/step - loss: 0.5348 - acc: 0.7329 - sigLoss: -580.7331 - significance: 24.2957 - asimovSignificance: 17.8226 - truePositive: 0.6343 - falsePositive: 0.1685 - val_loss: 0.5329 - val_acc: 0.7319 - val_sigLoss: -581.3994 - val_significance: 24.2496 - val_asimovSignificance: 17.8201 - val_truePositive: 0.6319 - val_falsePositive: 0.1682\n",
      "Epoch 15/200\n",
      "140000/140000 [==============================] - 0s 3us/step - loss: 0.5348 - acc: 0.7331 - sigLoss: -581.4686 - significance: 24.3489 - asimovSignificance: 17.7535 - truePositive: 0.6373 - falsePositive: 0.1710 - val_loss: 0.5329 - val_acc: 0.7317 - val_sigLoss: -581.7948 - val_significance: 24.2736 - val_asimovSignificance: 17.7452 - val_truePositive: 0.6333 - val_falsePositive: 0.1701\n",
      "Epoch 16/200\n",
      "140000/140000 [==============================] - 0s 3us/step - loss: 0.5342 - acc: 0.7330 - sigLoss: -582.3204 - significance: 24.3610 - asimovSignificance: 17.7163 - truePositive: 0.6379 - falsePositive: 0.1718 - val_loss: 0.5329 - val_acc: 0.7319 - val_sigLoss: -579.8596 - val_significance: 24.2696 - val_asimovSignificance: 17.7771 - val_truePositive: 0.6330 - val_falsePositive: 0.1694\n",
      "Epoch 17/200\n",
      "140000/140000 [==============================] - 0s 3us/step - loss: 0.5346 - acc: 0.7330 - sigLoss: -581.3717 - significance: 24.3784 - asimovSignificance: 17.6979 - truePositive: 0.6389 - falsePositive: 0.1728 - val_loss: 0.5331 - val_acc: 0.7318 - val_sigLoss: -581.5542 - val_significance: 24.3403 - val_asimovSignificance: 17.6383 - val_truePositive: 0.6369 - val_falsePositive: 0.1734\n",
      "Epoch 18/200\n",
      "140000/140000 [==============================] - 0s 3us/step - loss: 0.5341 - acc: 0.7331 - sigLoss: -581.8380 - significance: 24.3843 - asimovSignificance: 17.6948 - truePositive: 0.6392 - falsePositive: 0.1730 - val_loss: 0.5328 - val_acc: 0.7317 - val_sigLoss: -579.0427 - val_significance: 24.2861 - val_asimovSignificance: 17.7205 - val_truePositive: 0.6340 - val_falsePositive: 0.1708\n",
      "Epoch 19/200\n",
      "140000/140000 [==============================] - 0s 2us/step - loss: 0.5343 - acc: 0.7331 - sigLoss: -581.5297 - significance: 24.3681 - asimovSignificance: 17.7167 - truePositive: 0.6383 - falsePositive: 0.1720 - val_loss: 0.5329 - val_acc: 0.7317 - val_sigLoss: -581.5028 - val_significance: 24.3378 - val_asimovSignificance: 17.6300 - val_truePositive: 0.6368 - val_falsePositive: 0.1735\n",
      "Epoch 20/200\n",
      "140000/140000 [==============================] - 0s 3us/step - loss: 0.5337 - acc: 0.7331 - sigLoss: -581.6044 - significance: 24.3921 - asimovSignificance: 17.6825 - truePositive: 0.6396 - falsePositive: 0.1733 - val_loss: 0.5328 - val_acc: 0.7312 - val_sigLoss: -582.6656 - val_significance: 24.3609 - val_asimovSignificance: 17.5374 - val_truePositive: 0.6382 - val_falsePositive: 0.1760\n",
      "Epoch 21/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140000/140000 [==============================] - 0s 2us/step - loss: 0.5333 - acc: 0.7335 - sigLoss: -582.3152 - significance: 24.4388 - asimovSignificance: 17.6317 - truePositive: 0.6421 - falsePositive: 0.1751 - val_loss: 0.5328 - val_acc: 0.7310 - val_sigLoss: -580.6300 - val_significance: 24.3269 - val_asimovSignificance: 17.5794 - val_truePositive: 0.6363 - val_falsePositive: 0.1745\n",
      "Epoch 22/200\n",
      "140000/140000 [==============================] - 0s 3us/step - loss: 0.5343 - acc: 0.7331 - sigLoss: -581.7557 - significance: 24.4057 - asimovSignificance: 17.6497 - truePositive: 0.6404 - falsePositive: 0.1742 - val_loss: 0.5329 - val_acc: 0.7310 - val_sigLoss: -580.9290 - val_significance: 24.3377 - val_asimovSignificance: 17.5579 - val_truePositive: 0.6369 - val_falsePositive: 0.1750\n",
      "Epoch 23/200\n",
      "140000/140000 [==============================] - 0s 2us/step - loss: 0.5335 - acc: 0.7327 - sigLoss: -582.1022 - significance: 24.4299 - asimovSignificance: 17.5673 - truePositive: 0.6418 - falsePositive: 0.1764 - val_loss: 0.5328 - val_acc: 0.7311 - val_sigLoss: -578.5774 - val_significance: 24.2891 - val_asimovSignificance: 17.6624 - val_truePositive: 0.6342 - val_falsePositive: 0.1721\n",
      "Epoch 24/200\n",
      "140000/140000 [==============================] - 0s 2us/step - loss: 0.5337 - acc: 0.7330 - sigLoss: -581.2865 - significance: 24.4231 - asimovSignificance: 17.6086 - truePositive: 0.6414 - falsePositive: 0.1754 - val_loss: 0.5328 - val_acc: 0.7315 - val_sigLoss: -581.2228 - val_significance: 24.3479 - val_asimovSignificance: 17.5900 - val_truePositive: 0.6374 - val_falsePositive: 0.1745\n",
      "Epoch 25/200\n",
      "140000/140000 [==============================] - 0s 2us/step - loss: 0.5336 - acc: 0.7335 - sigLoss: -582.3660 - significance: 24.4193 - asimovSignificance: 17.6588 - truePositive: 0.6411 - falsePositive: 0.1740 - val_loss: 0.5328 - val_acc: 0.7312 - val_sigLoss: -580.9081 - val_significance: 24.3270 - val_asimovSignificance: 17.5959 - val_truePositive: 0.6363 - val_falsePositive: 0.1740\n",
      "Epoch 26/200\n",
      "140000/140000 [==============================] - 0s 3us/step - loss: 0.5336 - acc: 0.7333 - sigLoss: -581.9993 - significance: 24.4353 - asimovSignificance: 17.6173 - truePositive: 0.6420 - falsePositive: 0.1752 - val_loss: 0.5328 - val_acc: 0.7315 - val_sigLoss: -581.8521 - val_significance: 24.3613 - val_asimovSignificance: 17.5633 - val_truePositive: 0.6382 - val_falsePositive: 0.1753\n",
      "Epoch 27/200\n",
      "140000/140000 [==============================] - 0s 3us/step - loss: 0.5338 - acc: 0.7335 - sigLoss: -581.6648 - significance: 24.4231 - asimovSignificance: 17.6694 - truePositive: 0.6413 - falsePositive: 0.1744 - val_loss: 0.5328 - val_acc: 0.7313 - val_sigLoss: -581.6539 - val_significance: 24.3794 - val_asimovSignificance: 17.5145 - val_truePositive: 0.6392 - val_falsePositive: 0.1766\n",
      "Epoch 28/200\n",
      "140000/140000 [==============================] - 0s 3us/step - loss: 0.5337 - acc: 0.7329 - sigLoss: -582.3697 - significance: 24.4087 - asimovSignificance: 17.6332 - truePositive: 0.6406 - falsePositive: 0.1747 - val_loss: 0.5328 - val_acc: 0.7313 - val_sigLoss: -580.7532 - val_significance: 24.3384 - val_asimovSignificance: 17.5914 - val_truePositive: 0.6369 - val_falsePositive: 0.1743\n",
      "Epoch 29/200\n",
      "140000/140000 [==============================] - 0s 3us/step - loss: 0.5336 - acc: 0.7331 - sigLoss: -581.5458 - significance: 24.4051 - asimovSignificance: 17.6477 - truePositive: 0.6404 - falsePositive: 0.1742 - val_loss: 0.5328 - val_acc: 0.7311 - val_sigLoss: -581.1304 - val_significance: 24.3502 - val_asimovSignificance: 17.5445 - val_truePositive: 0.6376 - val_falsePositive: 0.1756\n",
      "Epoch 30/200\n",
      "140000/140000 [==============================] - 0s 3us/step - loss: 0.5333 - acc: 0.7334 - sigLoss: -581.4515 - significance: 24.4218 - asimovSignificance: 17.6385 - truePositive: 0.6412 - falsePositive: 0.1744 - val_loss: 0.5329 - val_acc: 0.7312 - val_sigLoss: -582.9804 - val_significance: 24.3833 - val_asimovSignificance: 17.4941 - val_truePositive: 0.6394 - val_falsePositive: 0.1772\n",
      "Epoch 31/200\n",
      "140000/140000 [==============================] - 0s 3us/step - loss: 0.5335 - acc: 0.7335 - sigLoss: -582.1652 - significance: 24.4308 - asimovSignificance: 17.6429 - truePositive: 0.6417 - falsePositive: 0.1746 - val_loss: 0.5328 - val_acc: 0.7311 - val_sigLoss: -581.1849 - val_significance: 24.3307 - val_asimovSignificance: 17.5837 - val_truePositive: 0.6365 - val_falsePositive: 0.1743\n",
      "Epoch 32/200\n",
      "140000/140000 [==============================] - 0s 3us/step - loss: 0.5335 - acc: 0.7331 - sigLoss: -582.4141 - significance: 24.3954 - asimovSignificance: 17.6791 - truePositive: 0.6398 - falsePositive: 0.1736 - val_loss: 0.5328 - val_acc: 0.7316 - val_sigLoss: -581.4987 - val_significance: 24.3472 - val_asimovSignificance: 17.5979 - val_truePositive: 0.6374 - val_falsePositive: 0.1743\n",
      "Epoch 33/200\n",
      "140000/140000 [==============================] - 0s 2us/step - loss: 0.5334 - acc: 0.7333 - sigLoss: -582.4734 - significance: 24.4259 - asimovSignificance: 17.6225 - truePositive: 0.6415 - falsePositive: 0.1750 - val_loss: 0.5329 - val_acc: 0.7313 - val_sigLoss: -581.9128 - val_significance: 24.3426 - val_asimovSignificance: 17.5771 - val_truePositive: 0.6372 - val_falsePositive: 0.1746\n",
      "Epoch 34/200\n",
      "140000/140000 [==============================] - 0s 3us/step - loss: 0.5330 - acc: 0.7339 - sigLoss: -581.4026 - significance: 24.4031 - asimovSignificance: 17.7421 - truePositive: 0.6401 - falsePositive: 0.1722 - val_loss: 0.5328 - val_acc: 0.7314 - val_sigLoss: -580.5592 - val_significance: 24.3275 - val_asimovSignificance: 17.6136 - val_truePositive: 0.6363 - val_falsePositive: 0.1736\n",
      "Epoch 35/200\n",
      "140000/140000 [==============================] - 0s 2us/step - loss: 0.5332 - acc: 0.7337 - sigLoss: -582.9515 - significance: 24.4154 - asimovSignificance: 17.6813 - truePositive: 0.6408 - falsePositive: 0.1734 - val_loss: 0.5329 - val_acc: 0.7311 - val_sigLoss: -577.8039 - val_significance: 24.3005 - val_asimovSignificance: 17.6385 - val_truePositive: 0.6349 - val_falsePositive: 0.1727\n",
      "Epoch 36/200\n",
      "140000/140000 [==============================] - 0s 3us/step - loss: 0.5336 - acc: 0.7335 - sigLoss: -581.4398 - significance: 24.3916 - asimovSignificance: 17.7105 - truePositive: 0.6396 - falsePositive: 0.1725 - val_loss: 0.5328 - val_acc: 0.7311 - val_sigLoss: -582.6176 - val_significance: 24.3997 - val_asimovSignificance: 17.4604 - val_truePositive: 0.6403 - val_falsePositive: 0.1782\n",
      "Epoch 37/200\n",
      "140000/140000 [==============================] - 0s 3us/step - loss: 0.5333 - acc: 0.7337 - sigLoss: -582.5438 - significance: 24.4470 - asimovSignificance: 17.6310 - truePositive: 0.6426 - falsePositive: 0.1752 - val_loss: 0.5328 - val_acc: 0.7313 - val_sigLoss: -582.2437 - val_significance: 24.3431 - val_asimovSignificance: 17.5738 - val_truePositive: 0.6372 - val_falsePositive: 0.1747\n",
      "Epoch 38/200\n",
      "140000/140000 [==============================] - 0s 3us/step - loss: 0.5333 - acc: 0.7335 - sigLoss: -582.1104 - significance: 24.4223 - asimovSignificance: 17.6432 - truePositive: 0.6413 - falsePositive: 0.1744 - val_loss: 0.5329 - val_acc: 0.7313 - val_sigLoss: -578.9030 - val_significance: 24.2559 - val_asimovSignificance: 17.7412 - val_truePositive: 0.6324 - val_falsePositive: 0.1698\n",
      "Epoch 39/200\n",
      "140000/140000 [==============================] - 0s 2us/step - loss: 0.5334 - acc: 0.7338 - sigLoss: -581.9894 - significance: 24.3913 - asimovSignificance: 17.7521 - truePositive: 0.6395 - falsePositive: 0.1719 - val_loss: 0.5328 - val_acc: 0.7313 - val_sigLoss: -578.8186 - val_significance: 24.2909 - val_asimovSignificance: 17.6741 - val_truePositive: 0.6343 - val_falsePositive: 0.1717\n",
      "Epoch 40/200\n",
      "140000/140000 [==============================] - 0s 3us/step - loss: 0.5331 - acc: 0.7338 - sigLoss: -582.0995 - significance: 24.3885 - asimovSignificance: 17.7624 - truePositive: 0.6393 - falsePositive: 0.1716 - val_loss: 0.5328 - val_acc: 0.7312 - val_sigLoss: -581.8028 - val_significance: 24.3827 - val_asimovSignificance: 17.4995 - val_truePositive: 0.6394 - val_falsePositive: 0.1771\n",
      "Epoch 41/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140000/140000 [==============================] - 0s 3us/step - loss: 0.5334 - acc: 0.7333 - sigLoss: -582.5384 - significance: 24.4366 - asimovSignificance: 17.6169 - truePositive: 0.6421 - falsePositive: 0.1753 - val_loss: 0.5328 - val_acc: 0.7317 - val_sigLoss: -579.5789 - val_significance: 24.3004 - val_asimovSignificance: 17.6942 - val_truePositive: 0.6348 - val_falsePositive: 0.1715\n",
      "Epoch 42/200\n",
      "140000/140000 [==============================] - 0s 3us/step - loss: 0.5335 - acc: 0.7335 - sigLoss: -581.6486 - significance: 24.3923 - asimovSignificance: 17.7077 - truePositive: 0.6396 - falsePositive: 0.1724 - val_loss: 0.5329 - val_acc: 0.7311 - val_sigLoss: -582.2425 - val_significance: 24.3166 - val_asimovSignificance: 17.6068 - val_truePositive: 0.6358 - val_falsePositive: 0.1736\n",
      "Epoch 43/200\n",
      "140000/140000 [==============================] - 0s 3us/step - loss: 0.5333 - acc: 0.7341 - sigLoss: -582.0566 - significance: 24.4003 - asimovSignificance: 17.7524 - truePositive: 0.6400 - falsePositive: 0.1717 - val_loss: 0.5329 - val_acc: 0.7310 - val_sigLoss: -580.8359 - val_significance: 24.3619 - val_asimovSignificance: 17.5204 - val_truePositive: 0.6383 - val_falsePositive: 0.1763\n",
      "Epoch 44/200\n",
      "140000/140000 [==============================] - 0s 3us/step - loss: 0.5332 - acc: 0.7333 - sigLoss: -582.3428 - significance: 24.4199 - asimovSignificance: 17.6426 - truePositive: 0.6411 - falsePositive: 0.1744 - val_loss: 0.5328 - val_acc: 0.7312 - val_sigLoss: -580.0679 - val_significance: 24.3201 - val_asimovSignificance: 17.6079 - val_truePositive: 0.6359 - val_falsePositive: 0.1736\n",
      "Epoch 45/200\n",
      "140000/140000 [==============================] - 0s 3us/step - loss: 0.5335 - acc: 0.7337 - sigLoss: -582.2068 - significance: 24.4291 - asimovSignificance: 17.6588 - truePositive: 0.6416 - falsePositive: 0.1742 - val_loss: 0.5328 - val_acc: 0.7312 - val_sigLoss: -579.2266 - val_significance: 24.2644 - val_asimovSignificance: 17.7206 - val_truePositive: 0.6329 - val_falsePositive: 0.1705\n",
      "Epoch 46/200\n",
      "140000/140000 [==============================] - 0s 3us/step - loss: 0.5337 - acc: 0.7337 - sigLoss: -580.8745 - significance: 24.3681 - asimovSignificance: 17.7661 - truePositive: 0.6383 - falsePositive: 0.1710 - val_loss: 0.5328 - val_acc: 0.7314 - val_sigLoss: -581.5686 - val_significance: 24.3820 - val_asimovSignificance: 17.5189 - val_truePositive: 0.6393 - val_falsePositive: 0.1766\n",
      "Epoch 47/200\n",
      "140000/140000 [==============================] - 0s 3us/step - loss: 0.5333 - acc: 0.7334 - sigLoss: -582.8427 - significance: 24.4451 - asimovSignificance: 17.6059 - truePositive: 0.6425 - falsePositive: 0.1756 - val_loss: 0.5328 - val_acc: 0.7313 - val_sigLoss: -580.4684 - val_significance: 24.3373 - val_asimovSignificance: 17.5931 - val_truePositive: 0.6369 - val_falsePositive: 0.1743\n",
      "Epoch 48/200\n",
      "140000/140000 [==============================] - 0s 3us/step - loss: 0.5330 - acc: 0.7339 - sigLoss: -581.5620 - significance: 24.3728 - asimovSignificance: 17.7942 - truePositive: 0.6384 - falsePositive: 0.1706 - val_loss: 0.5328 - val_acc: 0.7314 - val_sigLoss: -582.3669 - val_significance: 24.3532 - val_asimovSignificance: 17.5751 - val_truePositive: 0.6377 - val_falsePositive: 0.1749\n",
      "Epoch 49/200\n",
      "140000/140000 [==============================] - 0s 3us/step - loss: 0.5335 - acc: 0.7336 - sigLoss: -582.7044 - significance: 24.4365 - asimovSignificance: 17.6453 - truePositive: 0.6419 - falsePositive: 0.1746 - val_loss: 0.5329 - val_acc: 0.7314 - val_sigLoss: -576.0308 - val_significance: 24.2729 - val_asimovSignificance: 17.7209 - val_truePositive: 0.6333 - val_falsePositive: 0.1705\n",
      "Epoch 50/200\n",
      "140000/140000 [==============================] - 0s 3us/step - loss: 0.5333 - acc: 0.7333 - sigLoss: -581.6443 - significance: 24.4254 - asimovSignificance: 17.6401 - truePositive: 0.6415 - falsePositive: 0.1747 - val_loss: 0.5328 - val_acc: 0.7311 - val_sigLoss: -579.6555 - val_significance: 24.3397 - val_asimovSignificance: 17.5718 - val_truePositive: 0.6370 - val_falsePositive: 0.1748\n",
      "Epoch 51/200\n",
      "140000/140000 [==============================] - 0s 3us/step - loss: 0.5333 - acc: 0.7336 - sigLoss: -582.3908 - significance: 24.4142 - asimovSignificance: 17.6712 - truePositive: 0.6408 - falsePositive: 0.1736 - val_loss: 0.5329 - val_acc: 0.7315 - val_sigLoss: -577.4879 - val_significance: 24.3155 - val_asimovSignificance: 17.6465 - val_truePositive: 0.6356 - val_falsePositive: 0.1727\n",
      "Epoch 52/200\n",
      "140000/140000 [==============================] - 0s 3us/step - loss: 0.5334 - acc: 0.7336 - sigLoss: -581.2845 - significance: 24.4200 - asimovSignificance: 17.6702 - truePositive: 0.6411 - falsePositive: 0.1738 - val_loss: 0.5328 - val_acc: 0.7316 - val_sigLoss: -581.6710 - val_significance: 24.3442 - val_asimovSignificance: 17.6008 - val_truePositive: 0.6372 - val_falsePositive: 0.1742\n",
      "Epoch 53/200\n",
      "140000/140000 [==============================] - 0s 3us/step - loss: 0.5334 - acc: 0.7339 - sigLoss: -582.1938 - significance: 24.4403 - asimovSignificance: 17.6617 - truePositive: 0.6422 - falsePositive: 0.1743 - val_loss: 0.5329 - val_acc: 0.7315 - val_sigLoss: -583.3102 - val_significance: 24.3533 - val_asimovSignificance: 17.5789 - val_truePositive: 0.6377 - val_falsePositive: 0.1748\n",
      "Epoch 54/200\n",
      "140000/140000 [==============================] - 0s 3us/step - loss: 0.5340 - acc: 0.7336 - sigLoss: -581.5238 - significance: 24.4153 - asimovSignificance: 17.6702 - truePositive: 0.6409 - falsePositive: 0.1738 - val_loss: 0.5329 - val_acc: 0.7315 - val_sigLoss: -579.6734 - val_significance: 24.2955 - val_asimovSignificance: 17.6850 - val_truePositive: 0.6345 - val_falsePositive: 0.1716\n",
      "Epoch 55/200\n",
      "140000/140000 [==============================] - 0s 3us/step - loss: 0.5334 - acc: 0.7337 - sigLoss: -582.1366 - significance: 24.4333 - asimovSignificance: 17.6563 - truePositive: 0.6419 - falsePositive: 0.1744 - val_loss: 0.5329 - val_acc: 0.7313 - val_sigLoss: -581.3796 - val_significance: 24.3712 - val_asimovSignificance: 17.5319 - val_truePositive: 0.6387 - val_falsePositive: 0.1762\n",
      "Epoch 56/200\n",
      "140000/140000 [==============================] - 0s 3us/step - loss: 0.5330 - acc: 0.7337 - sigLoss: -582.1603 - significance: 24.3920 - asimovSignificance: 17.7315 - truePositive: 0.6395 - falsePositive: 0.1721 - val_loss: 0.5329 - val_acc: 0.7312 - val_sigLoss: -578.0470 - val_significance: 24.3140 - val_asimovSignificance: 17.6249 - val_truePositive: 0.6356 - val_falsePositive: 0.1732\n",
      "Epoch 57/200\n",
      "140000/140000 [==============================] - 0s 3us/step - loss: 0.5329 - acc: 0.7338 - sigLoss: -581.9282 - significance: 24.3987 - asimovSignificance: 17.7430 - truePositive: 0.6399 - falsePositive: 0.1724 - val_loss: 0.5329 - val_acc: 0.7317 - val_sigLoss: -579.9990 - val_significance: 24.3014 - val_asimovSignificance: 17.6891 - val_truePositive: 0.6348 - val_falsePositive: 0.1716\n",
      "Epoch 58/200\n",
      "140000/140000 [==============================] - 0s 3us/step - loss: 0.5337 - acc: 0.7337 - sigLoss: -581.2776 - significance: 24.3848 - asimovSignificance: 17.7434 - truePositive: 0.6391 - falsePositive: 0.1717 - val_loss: 0.5328 - val_acc: 0.7319 - val_sigLoss: -580.7706 - val_significance: 24.4012 - val_asimovSignificance: 17.5308 - val_truePositive: 0.6403 - val_falsePositive: 0.1767\n",
      "Epoch 59/200\n",
      "140000/140000 [==============================] - 0s 3us/step - loss: 0.5336 - acc: 0.7338 - sigLoss: -581.4926 - significance: 24.4121 - asimovSignificance: 17.7108 - truePositive: 0.6406 - falsePositive: 0.1729 - val_loss: 0.5329 - val_acc: 0.7313 - val_sigLoss: -583.0212 - val_significance: 24.4708 - val_asimovSignificance: 17.3574 - val_truePositive: 0.6443 - val_falsePositive: 0.1817\n",
      "Epoch 60/200\n",
      "140000/140000 [==============================] - 0s 3us/step - loss: 0.5334 - acc: 0.7333 - sigLoss: -582.1535 - significance: 24.4410 - asimovSignificance: 17.6100 - truePositive: 0.6423 - falsePositive: 0.1756 - val_loss: 0.5329 - val_acc: 0.7313 - val_sigLoss: -584.5133 - val_significance: 24.3848 - val_asimovSignificance: 17.5006 - val_truePositive: 0.6395 - val_falsePositive: 0.1770\n",
      "Epoch 61/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140000/140000 [==============================] - 0s 3us/step - loss: 0.5329 - acc: 0.7334 - sigLoss: -583.1979 - significance: 24.4217 - asimovSignificance: 17.6395 - truePositive: 0.6413 - falsePositive: 0.1745 - val_loss: 0.5329 - val_acc: 0.7319 - val_sigLoss: -577.7551 - val_significance: 24.3128 - val_asimovSignificance: 17.6932 - val_truePositive: 0.6354 - val_falsePositive: 0.1718\n",
      "Epoch 62/200\n",
      "140000/140000 [==============================] - 0s 3us/step - loss: 0.5335 - acc: 0.7332 - sigLoss: -580.9313 - significance: 24.3832 - asimovSignificance: 17.7009 - truePositive: 0.6391 - falsePositive: 0.1728 - val_loss: 0.5328 - val_acc: 0.7314 - val_sigLoss: -580.9965 - val_significance: 24.3507 - val_asimovSignificance: 17.5770 - val_truePositive: 0.6376 - val_falsePositive: 0.1748\n",
      "Epoch 63/200\n",
      "140000/140000 [==============================] - 0s 3us/step - loss: 0.5329 - acc: 0.7338 - sigLoss: -582.7865 - significance: 24.4646 - asimovSignificance: 17.6169 - truePositive: 0.6436 - falsePositive: 0.1759 - val_loss: 0.5329 - val_acc: 0.7314 - val_sigLoss: -582.2092 - val_significance: 24.3200 - val_asimovSignificance: 17.6350 - val_truePositive: 0.6359 - val_falsePositive: 0.1731\n",
      "Epoch 64/200\n",
      "140000/140000 [==============================] - 0s 3us/step - loss: 0.5328 - acc: 0.7338 - sigLoss: -582.3664 - significance: 24.4230 - asimovSignificance: 17.6785 - truePositive: 0.6412 - falsePositive: 0.1738 - val_loss: 0.5329 - val_acc: 0.7313 - val_sigLoss: -581.7243 - val_significance: 24.3881 - val_asimovSignificance: 17.5016 - val_truePositive: 0.6397 - val_falsePositive: 0.1771\n",
      "Epoch 65/200\n",
      "140000/140000 [==============================] - 0s 3us/step - loss: 0.5329 - acc: 0.7335 - sigLoss: -582.4950 - significance: 24.4243 - asimovSignificance: 17.6534 - truePositive: 0.6414 - falsePositive: 0.1743 - val_loss: 0.5328 - val_acc: 0.7316 - val_sigLoss: -579.9460 - val_significance: 24.3516 - val_asimovSignificance: 17.5937 - val_truePositive: 0.6376 - val_falsePositive: 0.1745\n",
      "Epoch 66/200\n",
      "140000/140000 [==============================] - 0s 3us/step - loss: 0.5327 - acc: 0.7338 - sigLoss: -581.4981 - significance: 24.4269 - asimovSignificance: 17.6755 - truePositive: 0.6415 - falsePositive: 0.1739 - val_loss: 0.5328 - val_acc: 0.7317 - val_sigLoss: -581.7777 - val_significance: 24.3579 - val_asimovSignificance: 17.5871 - val_truePositive: 0.6380 - val_falsePositive: 0.1747\n",
      "Epoch 67/200\n",
      "140000/140000 [==============================] - 0s 3us/step - loss: 0.5334 - acc: 0.7338 - sigLoss: -581.8024 - significance: 24.4218 - asimovSignificance: 17.6816 - truePositive: 0.6412 - falsePositive: 0.1736 - val_loss: 0.5328 - val_acc: 0.7315 - val_sigLoss: -581.6725 - val_significance: 24.3472 - val_asimovSignificance: 17.5935 - val_truePositive: 0.6374 - val_falsePositive: 0.1744\n",
      "Epoch 68/200\n",
      "140000/140000 [==============================] - 0s 3us/step - loss: 0.5337 - acc: 0.7331 - sigLoss: -582.2666 - significance: 24.4208 - asimovSignificance: 17.6065 - truePositive: 0.6412 - falsePositive: 0.1751 - val_loss: 0.5329 - val_acc: 0.7316 - val_sigLoss: -578.4869 - val_significance: 24.3369 - val_asimovSignificance: 17.6266 - val_truePositive: 0.6368 - val_falsePositive: 0.1736\n",
      "Epoch 69/200\n",
      "140000/140000 [==============================] - 0s 3us/step - loss: 0.5333 - acc: 0.7339 - sigLoss: -582.1632 - significance: 24.4487 - asimovSignificance: 17.6512 - truePositive: 0.6426 - falsePositive: 0.1748 - val_loss: 0.5329 - val_acc: 0.7316 - val_sigLoss: -578.2778 - val_significance: 24.2570 - val_asimovSignificance: 17.7689 - val_truePositive: 0.6324 - val_falsePositive: 0.1693\n",
      "Epoch 70/200\n",
      "140000/140000 [==============================] - 0s 3us/step - loss: 0.5336 - acc: 0.7336 - sigLoss: -581.3343 - significance: 24.3881 - asimovSignificance: 17.7350 - truePositive: 0.6394 - falsePositive: 0.1722 - val_loss: 0.5328 - val_acc: 0.7312 - val_sigLoss: -580.7916 - val_significance: 24.3929 - val_asimovSignificance: 17.4822 - val_truePositive: 0.6400 - val_falsePositive: 0.1776\n",
      "Epoch 71/200\n",
      "140000/140000 [==============================] - 0s 3us/step - loss: 0.5333 - acc: 0.7339 - sigLoss: -581.9980 - significance: 24.4506 - asimovSignificance: 17.6571 - truePositive: 0.6427 - falsePositive: 0.1750 - val_loss: 0.5329 - val_acc: 0.7318 - val_sigLoss: -582.4171 - val_significance: 24.3313 - val_asimovSignificance: 17.6506 - val_truePositive: 0.6365 - val_falsePositive: 0.1729\n",
      "Epoch 72/200\n",
      "140000/140000 [==============================] - 0s 3us/step - loss: 0.5332 - acc: 0.7336 - sigLoss: -581.9169 - significance: 24.4047 - asimovSignificance: 17.6910 - truePositive: 0.6403 - falsePositive: 0.1731 - val_loss: 0.5328 - val_acc: 0.7312 - val_sigLoss: -582.0884 - val_significance: 24.3616 - val_asimovSignificance: 17.5394 - val_truePositive: 0.6382 - val_falsePositive: 0.1759\n",
      "Epoch 73/200\n",
      "140000/140000 [==============================] - 0s 3us/step - loss: 0.5334 - acc: 0.7338 - sigLoss: -581.6387 - significance: 24.4100 - asimovSignificance: 17.7159 - truePositive: 0.6405 - falsePositive: 0.1730 - val_loss: 0.5328 - val_acc: 0.7312 - val_sigLoss: -582.4444 - val_significance: 24.4439 - val_asimovSignificance: 17.3906 - val_truePositive: 0.6428 - val_falsePositive: 0.1804\n",
      "Epoch 74/200\n",
      "140000/140000 [==============================] - 0s 3us/step - loss: 0.5334 - acc: 0.7338 - sigLoss: -582.2974 - significance: 24.4564 - asimovSignificance: 17.6219 - truePositive: 0.6431 - falsePositive: 0.1756 - val_loss: 0.5329 - val_acc: 0.7315 - val_sigLoss: -579.8289 - val_significance: 24.3365 - val_asimovSignificance: 17.6098 - val_truePositive: 0.6368 - val_falsePositive: 0.1739\n",
      "Epoch 75/200\n",
      "140000/140000 [==============================] - 0s 3us/step - loss: 0.5336 - acc: 0.7337 - sigLoss: -581.4809 - significance: 24.4231 - asimovSignificance: 17.6806 - truePositive: 0.6413 - falsePositive: 0.1738 - val_loss: 0.5329 - val_acc: 0.7313 - val_sigLoss: -581.9028 - val_significance: 24.3413 - val_asimovSignificance: 17.5838 - val_truePositive: 0.6371 - val_falsePositive: 0.1745\n",
      "Epoch 76/200\n",
      "140000/140000 [==============================] - 0s 3us/step - loss: 0.5329 - acc: 0.7336 - sigLoss: -583.2101 - significance: 24.4473 - asimovSignificance: 17.6288 - truePositive: 0.6426 - falsePositive: 0.1753 - val_loss: 0.5329 - val_acc: 0.7318 - val_sigLoss: -579.3417 - val_significance: 24.3054 - val_asimovSignificance: 17.7008 - val_truePositive: 0.6350 - val_falsePositive: 0.1714\n",
      "Epoch 77/200\n",
      "140000/140000 [==============================] - 0s 3us/step - loss: 0.5336 - acc: 0.7339 - sigLoss: -581.4215 - significance: 24.4324 - asimovSignificance: 17.6882 - truePositive: 0.6417 - falsePositive: 0.1737 - val_loss: 0.5329 - val_acc: 0.7314 - val_sigLoss: -583.7558 - val_significance: 24.3938 - val_asimovSignificance: 17.4935 - val_truePositive: 0.6400 - val_falsePositive: 0.1773\n",
      "Epoch 78/200\n",
      "140000/140000 [==============================] - 0s 3us/step - loss: 0.5333 - acc: 0.7337 - sigLoss: -582.0813 - significance: 24.4329 - asimovSignificance: 17.6566 - truePositive: 0.6418 - falsePositive: 0.1743 - val_loss: 0.5328 - val_acc: 0.7316 - val_sigLoss: -581.6779 - val_significance: 24.3696 - val_asimovSignificance: 17.5615 - val_truePositive: 0.6386 - val_falsePositive: 0.1755\n",
      "Epoch 79/200\n",
      "140000/140000 [==============================] - 0s 3us/step - loss: 0.5330 - acc: 0.7339 - sigLoss: -582.4488 - significance: 24.4120 - asimovSignificance: 17.7325 - truePositive: 0.6406 - falsePositive: 0.1729 - val_loss: 0.5328 - val_acc: 0.7316 - val_sigLoss: -578.4673 - val_significance: 24.3121 - val_asimovSignificance: 17.6710 - val_truePositive: 0.6354 - val_falsePositive: 0.1723\n",
      "Epoch 80/200\n",
      "140000/140000 [==============================] - 0s 3us/step - loss: 0.5333 - acc: 0.7336 - sigLoss: -581.8669 - significance: 24.4047 - asimovSignificance: 17.6949 - truePositive: 0.6403 - falsePositive: 0.1731 - val_loss: 0.5328 - val_acc: 0.7313 - val_sigLoss: -582.5287 - val_significance: 24.4319 - val_asimovSignificance: 17.4189 - val_truePositive: 0.6421 - val_falsePositive: 0.1797\n",
      "Epoch 81/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140000/140000 [==============================] - 0s 3us/step - loss: 0.5332 - acc: 0.7333 - sigLoss: -581.9447 - significance: 24.4358 - asimovSignificance: 17.6184 - truePositive: 0.6420 - falsePositive: 0.1753 - val_loss: 0.5330 - val_acc: 0.7316 - val_sigLoss: -582.6660 - val_significance: 24.3216 - val_asimovSignificance: 17.6522 - val_truePositive: 0.6360 - val_falsePositive: 0.1728\n",
      "Epoch 82/200\n",
      "140000/140000 [==============================] - 0s 3us/step - loss: 0.5337 - acc: 0.7339 - sigLoss: -581.9450 - significance: 24.3931 - asimovSignificance: 17.7612 - truePositive: 0.6395 - falsePositive: 0.1715 - val_loss: 0.5328 - val_acc: 0.7313 - val_sigLoss: -582.4481 - val_significance: 24.3928 - val_asimovSignificance: 17.4876 - val_truePositive: 0.6399 - val_falsePositive: 0.1775\n",
      "Epoch 83/200\n",
      "140000/140000 [==============================] - 0s 3us/step - loss: 0.5335 - acc: 0.7340 - sigLoss: -581.8231 - significance: 24.4330 - asimovSignificance: 17.6915 - truePositive: 0.6418 - falsePositive: 0.1736 - val_loss: 0.5328 - val_acc: 0.7314 - val_sigLoss: -582.3304 - val_significance: 24.3511 - val_asimovSignificance: 17.5732 - val_truePositive: 0.6376 - val_falsePositive: 0.1749\n",
      "Epoch 84/200\n",
      "140000/140000 [==============================] - 0s 3us/step - loss: 0.5338 - acc: 0.7335 - sigLoss: -581.7351 - significance: 24.3998 - asimovSignificance: 17.7066 - truePositive: 0.6400 - falsePositive: 0.1729 - val_loss: 0.5329 - val_acc: 0.7316 - val_sigLoss: -582.2048 - val_significance: 24.3241 - val_asimovSignificance: 17.6483 - val_truePositive: 0.6361 - val_falsePositive: 0.1729\n",
      "Epoch 85/200\n",
      "140000/140000 [==============================] - 0s 3us/step - loss: 0.5331 - acc: 0.7333 - sigLoss: -581.9640 - significance: 24.4016 - asimovSignificance: 17.6694 - truePositive: 0.6402 - falsePositive: 0.1737 - val_loss: 0.5328 - val_acc: 0.7313 - val_sigLoss: -581.2791 - val_significance: 24.3601 - val_asimovSignificance: 17.5506 - val_truePositive: 0.6381 - val_falsePositive: 0.1755\n",
      "Epoch 86/200\n",
      "140000/140000 [==============================] - 0s 3us/step - loss: 0.5333 - acc: 0.7335 - sigLoss: -581.3499 - significance: 24.3690 - asimovSignificance: 17.7645 - truePositive: 0.6383 - falsePositive: 0.1712 - val_loss: 0.5328 - val_acc: 0.7308 - val_sigLoss: -580.6996 - val_significance: 24.3968 - val_asimovSignificance: 17.4360 - val_truePositive: 0.6402 - val_falsePositive: 0.1787\n",
      "Epoch 87/200\n",
      "140000/140000 [==============================] - 0s 3us/step - loss: 0.5336 - acc: 0.7336 - sigLoss: -582.4412 - significance: 24.4277 - asimovSignificance: 17.6566 - truePositive: 0.6415 - falsePositive: 0.1745 - val_loss: 0.5328 - val_acc: 0.7318 - val_sigLoss: -579.4609 - val_significance: 24.3822 - val_asimovSignificance: 17.5550 - val_truePositive: 0.6393 - val_falsePositive: 0.1758\n",
      "Epoch 88/200\n",
      "140000/140000 [==============================] - 0s 3us/step - loss: 0.5332 - acc: 0.7335 - sigLoss: -581.7254 - significance: 24.4182 - asimovSignificance: 17.6696 - truePositive: 0.6410 - falsePositive: 0.1740 - val_loss: 0.5329 - val_acc: 0.7317 - val_sigLoss: -577.3051 - val_significance: 24.2977 - val_asimovSignificance: 17.7045 - val_truePositive: 0.6346 - val_falsePositive: 0.1713\n",
      "Epoch 89/200\n",
      "140000/140000 [==============================] - 0s 3us/step - loss: 0.5336 - acc: 0.7335 - sigLoss: -580.9472 - significance: 24.3670 - asimovSignificance: 17.7663 - truePositive: 0.6381 - falsePositive: 0.1711 - val_loss: 0.5329 - val_acc: 0.7317 - val_sigLoss: -582.7513 - val_significance: 24.4220 - val_asimovSignificance: 17.4821 - val_truePositive: 0.6415 - val_falsePositive: 0.1781\n",
      "Epoch 90/200\n",
      "140000/140000 [==============================] - 0s 3us/step - loss: 0.5332 - acc: 0.7336 - sigLoss: -582.4991 - significance: 24.4025 - asimovSignificance: 17.6996 - truePositive: 0.6401 - falsePositive: 0.1729 - val_loss: 0.5328 - val_acc: 0.7312 - val_sigLoss: -580.1525 - val_significance: 24.3781 - val_asimovSignificance: 17.5093 - val_truePositive: 0.6391 - val_falsePositive: 0.1768\n",
      "Epoch 91/200\n",
      "140000/140000 [==============================] - 0s 3us/step - loss: 0.5330 - acc: 0.7333 - sigLoss: -581.9311 - significance: 24.4073 - asimovSignificance: 17.6627 - truePositive: 0.6405 - falsePositive: 0.1738 - val_loss: 0.5328 - val_acc: 0.7316 - val_sigLoss: -582.3383 - val_significance: 24.3874 - val_asimovSignificance: 17.5291 - val_truePositive: 0.6396 - val_falsePositive: 0.1764\n",
      "Epoch 92/200\n",
      "140000/140000 [==============================] - 0s 3us/step - loss: 0.5338 - acc: 0.7338 - sigLoss: -581.8017 - significance: 24.4339 - asimovSignificance: 17.6527 - truePositive: 0.6419 - falsePositive: 0.1743 - val_loss: 0.5329 - val_acc: 0.7315 - val_sigLoss: -581.7698 - val_significance: 24.3773 - val_asimovSignificance: 17.5392 - val_truePositive: 0.6391 - val_falsePositive: 0.1761\n",
      "Epoch 93/200\n",
      "140000/140000 [==============================] - 0s 3us/step - loss: 0.5330 - acc: 0.7332 - sigLoss: -582.0697 - significance: 24.3885 - asimovSignificance: 17.6774 - truePositive: 0.6394 - falsePositive: 0.1732 - val_loss: 0.5328 - val_acc: 0.7317 - val_sigLoss: -580.1175 - val_significance: 24.3621 - val_asimovSignificance: 17.5829 - val_truePositive: 0.6382 - val_falsePositive: 0.1749\n",
      "Epoch 94/200\n",
      "140000/140000 [==============================] - 0s 3us/step - loss: 0.5330 - acc: 0.7336 - sigLoss: -581.9896 - significance: 24.3996 - asimovSignificance: 17.6921 - truePositive: 0.6400 - falsePositive: 0.1729 - val_loss: 0.5328 - val_acc: 0.7316 - val_sigLoss: -580.3241 - val_significance: 24.3424 - val_asimovSignificance: 17.6054 - val_truePositive: 0.6371 - val_falsePositive: 0.1741\n",
      "Epoch 95/200\n",
      "140000/140000 [==============================] - 0s 3us/step - loss: 0.5335 - acc: 0.7336 - sigLoss: -581.6721 - significance: 24.4244 - asimovSignificance: 17.6809 - truePositive: 0.6413 - falsePositive: 0.1740 - val_loss: 0.5329 - val_acc: 0.7316 - val_sigLoss: -582.4073 - val_significance: 24.3076 - val_asimovSignificance: 17.6735 - val_truePositive: 0.6352 - val_falsePositive: 0.1721\n",
      "Epoch 96/200\n",
      "140000/140000 [==============================] - 0s 3us/step - loss: 0.5329 - acc: 0.7338 - sigLoss: -582.1012 - significance: 24.3993 - asimovSignificance: 17.7221 - truePositive: 0.6399 - falsePositive: 0.1724 - val_loss: 0.5329 - val_acc: 0.7311 - val_sigLoss: -581.9614 - val_significance: 24.3797 - val_asimovSignificance: 17.4900 - val_truePositive: 0.6393 - val_falsePositive: 0.1772\n",
      "Epoch 97/200\n",
      "140000/140000 [==============================] - 0s 3us/step - loss: 0.5333 - acc: 0.7335 - sigLoss: -581.8805 - significance: 24.4209 - asimovSignificance: 17.6841 - truePositive: 0.6412 - falsePositive: 0.1740 - val_loss: 0.5329 - val_acc: 0.7315 - val_sigLoss: -581.9020 - val_significance: 24.3147 - val_asimovSignificance: 17.6569 - val_truePositive: 0.6356 - val_falsePositive: 0.1726\n",
      "Epoch 98/200\n",
      "140000/140000 [==============================] - 0s 3us/step - loss: 0.5333 - acc: 0.7336 - sigLoss: -582.6700 - significance: 24.4302 - asimovSignificance: 17.6646 - truePositive: 0.6417 - falsePositive: 0.1743 - val_loss: 0.5329 - val_acc: 0.7318 - val_sigLoss: -578.0632 - val_significance: 24.2829 - val_asimovSignificance: 17.7383 - val_truePositive: 0.6338 - val_falsePositive: 0.1703\n",
      "Epoch 99/200\n",
      "140000/140000 [==============================] - 0s 3us/step - loss: 0.5336 - acc: 0.7341 - sigLoss: -581.6661 - significance: 24.4292 - asimovSignificance: 17.7055 - truePositive: 0.6415 - falsePositive: 0.1733 - val_loss: 0.5329 - val_acc: 0.7312 - val_sigLoss: -582.6724 - val_significance: 24.4094 - val_asimovSignificance: 17.4508 - val_truePositive: 0.6409 - val_falsePositive: 0.1785\n",
      "Epoch 100/200\n",
      "140000/140000 [==============================] - 0s 3us/step - loss: 0.5330 - acc: 0.7334 - sigLoss: -582.2079 - significance: 24.4214 - asimovSignificance: 17.6562 - truePositive: 0.6412 - falsePositive: 0.1743 - val_loss: 0.5330 - val_acc: 0.7316 - val_sigLoss: -579.9117 - val_significance: 24.2805 - val_asimovSignificance: 17.7307 - val_truePositive: 0.6337 - val_falsePositive: 0.1705\n",
      "Epoch 101/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140000/140000 [==============================] - 0s 3us/step - loss: 0.5332 - acc: 0.7338 - sigLoss: -582.3249 - significance: 24.4082 - asimovSignificance: 17.7179 - truePositive: 0.6404 - falsePositive: 0.1728 - val_loss: 0.5329 - val_acc: 0.7316 - val_sigLoss: -578.8154 - val_significance: 24.3433 - val_asimovSignificance: 17.6128 - val_truePositive: 0.6372 - val_falsePositive: 0.1740\n",
      "Epoch 102/200\n",
      "140000/140000 [==============================] - 0s 3us/step - loss: 0.5332 - acc: 0.7338 - sigLoss: -581.1977 - significance: 24.4046 - asimovSignificance: 17.7184 - truePositive: 0.6402 - falsePositive: 0.1727 - val_loss: 0.5328 - val_acc: 0.7311 - val_sigLoss: -581.9313 - val_significance: 24.3654 - val_asimovSignificance: 17.5242 - val_truePositive: 0.6385 - val_falsePositive: 0.1762\n",
      "Epoch 103/200\n",
      "140000/140000 [==============================] - 0s 3us/step - loss: 0.5331 - acc: 0.7337 - sigLoss: -582.3476 - significance: 24.4143 - asimovSignificance: 17.6984 - truePositive: 0.6408 - falsePositive: 0.1733 - val_loss: 0.5328 - val_acc: 0.7318 - val_sigLoss: -579.3382 - val_significance: 24.3227 - val_asimovSignificance: 17.6641 - val_truePositive: 0.6360 - val_falsePositive: 0.1725\n",
      "Epoch 104/200\n",
      "140000/140000 [==============================] - 0s 3us/step - loss: 0.5334 - acc: 0.7336 - sigLoss: -581.3633 - significance: 24.4189 - asimovSignificance: 17.6707 - truePositive: 0.6410 - falsePositive: 0.1738 - val_loss: 0.5329 - val_acc: 0.7313 - val_sigLoss: -583.5371 - val_significance: 24.3623 - val_asimovSignificance: 17.5485 - val_truePositive: 0.6383 - val_falsePositive: 0.1757\n",
      "Epoch 105/200\n",
      "140000/140000 [==============================] - 0s 3us/step - loss: 0.5333 - acc: 0.7335 - sigLoss: -581.7907 - significance: 24.3869 - asimovSignificance: 17.7176 - truePositive: 0.6393 - falsePositive: 0.1722 - val_loss: 0.5329 - val_acc: 0.7311 - val_sigLoss: -583.8440 - val_significance: 24.3851 - val_asimovSignificance: 17.4824 - val_truePositive: 0.6396 - val_falsePositive: 0.1775\n",
      "Epoch 106/200\n",
      "140000/140000 [==============================] - 0s 3us/step - loss: 0.5335 - acc: 0.7342 - sigLoss: -582.2094 - significance: 24.4266 - asimovSignificance: 17.7264 - truePositive: 0.6413 - falsePositive: 0.1730 - val_loss: 0.5328 - val_acc: 0.7316 - val_sigLoss: -579.7349 - val_significance: 24.3209 - val_asimovSignificance: 17.6521 - val_truePositive: 0.6359 - val_falsePositive: 0.1728\n",
      "Epoch 107/200\n",
      "140000/140000 [==============================] - 0s 3us/step - loss: 0.5333 - acc: 0.7337 - sigLoss: -581.8303 - significance: 24.4128 - asimovSignificance: 17.6825 - truePositive: 0.6407 - falsePositive: 0.1735 - val_loss: 0.5329 - val_acc: 0.7318 - val_sigLoss: -579.0020 - val_significance: 24.2808 - val_asimovSignificance: 17.7450 - val_truePositive: 0.6337 - val_falsePositive: 0.1701\n",
      "Epoch 108/200\n",
      "140000/140000 [==============================] - 0s 3us/step - loss: 0.5336 - acc: 0.7337 - sigLoss: -581.6223 - significance: 24.4112 - asimovSignificance: 17.7110 - truePositive: 0.6406 - falsePositive: 0.1730 - val_loss: 0.5330 - val_acc: 0.7314 - val_sigLoss: -581.0085 - val_significance: 24.3521 - val_asimovSignificance: 17.5717 - val_truePositive: 0.6377 - val_falsePositive: 0.1750\n",
      "Epoch 109/200\n",
      "140000/140000 [==============================] - 0s 3us/step - loss: 0.5332 - acc: 0.7333 - sigLoss: -581.8003 - significance: 24.4074 - asimovSignificance: 17.6743 - truePositive: 0.6404 - falsePositive: 0.1739 - val_loss: 0.5328 - val_acc: 0.7310 - val_sigLoss: -582.6904 - val_significance: 24.3825 - val_asimovSignificance: 17.4793 - val_truePositive: 0.6394 - val_falsePositive: 0.1775\n",
      "Epoch 110/200\n",
      "140000/140000 [==============================] - 0s 3us/step - loss: 0.5330 - acc: 0.7336 - sigLoss: -582.0110 - significance: 24.3831 - asimovSignificance: 17.7399 - truePositive: 0.6391 - falsePositive: 0.1719 - val_loss: 0.5328 - val_acc: 0.7310 - val_sigLoss: -582.8444 - val_significance: 24.4334 - val_asimovSignificance: 17.3876 - val_truePositive: 0.6422 - val_falsePositive: 0.1804\n",
      "Epoch 111/200\n",
      "140000/140000 [==============================] - 0s 3us/step - loss: 0.5336 - acc: 0.7338 - sigLoss: -581.9368 - significance: 24.4413 - asimovSignificance: 17.6592 - truePositive: 0.6423 - falsePositive: 0.1746 - val_loss: 0.5329 - val_acc: 0.7319 - val_sigLoss: -581.8347 - val_significance: 24.2981 - val_asimovSignificance: 17.7181 - val_truePositive: 0.6346 - val_falsePositive: 0.1710\n",
      "Epoch 112/200\n",
      "140000/140000 [==============================] - 0s 3us/step - loss: 0.5328 - acc: 0.7338 - sigLoss: -582.3909 - significance: 24.4036 - asimovSignificance: 17.7326 - truePositive: 0.6402 - falsePositive: 0.1726 - val_loss: 0.5329 - val_acc: 0.7315 - val_sigLoss: -580.6111 - val_significance: 24.3363 - val_asimovSignificance: 17.6160 - val_truePositive: 0.6368 - val_falsePositive: 0.1738\n",
      "Epoch 113/200\n",
      "140000/140000 [==============================] - 0s 3us/step - loss: 0.5336 - acc: 0.7338 - sigLoss: -581.4828 - significance: 24.3727 - asimovSignificance: 17.7901 - truePositive: 0.6385 - falsePositive: 0.1709 - val_loss: 0.5328 - val_acc: 0.7315 - val_sigLoss: -582.3912 - val_significance: 24.3834 - val_asimovSignificance: 17.5272 - val_truePositive: 0.6394 - val_falsePositive: 0.1765\n",
      "Epoch 114/200\n",
      "140000/140000 [==============================] - 0s 3us/step - loss: 0.5332 - acc: 0.7334 - sigLoss: -582.4599 - significance: 24.4453 - asimovSignificance: 17.6123 - truePositive: 0.6425 - falsePositive: 0.1757 - val_loss: 0.5328 - val_acc: 0.7316 - val_sigLoss: -579.4897 - val_significance: 24.3012 - val_asimovSignificance: 17.6879 - val_truePositive: 0.6348 - val_falsePositive: 0.1717\n",
      "Epoch 115/200\n",
      "140000/140000 [==============================] - 0s 3us/step - loss: 0.5333 - acc: 0.7338 - sigLoss: -581.3142 - significance: 24.3812 - asimovSignificance: 17.7578 - truePositive: 0.6389 - falsePositive: 0.1715 - val_loss: 0.5329 - val_acc: 0.7315 - val_sigLoss: -581.7754 - val_significance: 24.3647 - val_asimovSignificance: 17.5623 - val_truePositive: 0.6384 - val_falsePositive: 0.1754\n",
      "Epoch 116/200\n",
      "140000/140000 [==============================] - 0s 3us/step - loss: 0.5337 - acc: 0.7340 - sigLoss: -582.2531 - significance: 24.4152 - asimovSignificance: 17.7296 - truePositive: 0.6408 - falsePositive: 0.1727 - val_loss: 0.5329 - val_acc: 0.7317 - val_sigLoss: -580.4759 - val_significance: 24.3539 - val_asimovSignificance: 17.6021 - val_truePositive: 0.6377 - val_falsePositive: 0.1743\n",
      "Epoch 117/200\n",
      "140000/140000 [==============================] - 0s 3us/step - loss: 0.5338 - acc: 0.7336 - sigLoss: -581.2312 - significance: 24.4080 - asimovSignificance: 17.6870 - truePositive: 0.6404 - falsePositive: 0.1731 - val_loss: 0.5328 - val_acc: 0.7318 - val_sigLoss: -583.2187 - val_significance: 24.3435 - val_asimovSignificance: 17.6279 - val_truePositive: 0.6371 - val_falsePositive: 0.1736\n",
      "Epoch 118/200\n",
      "140000/140000 [==============================] - 0s 3us/step - loss: 0.5336 - acc: 0.7335 - sigLoss: -581.7274 - significance: 24.4129 - asimovSignificance: 17.6823 - truePositive: 0.6407 - falsePositive: 0.1737 - val_loss: 0.5329 - val_acc: 0.7317 - val_sigLoss: -581.8181 - val_significance: 24.3857 - val_asimovSignificance: 17.5376 - val_truePositive: 0.6395 - val_falsePositive: 0.1762\n",
      "Epoch 119/200\n",
      "140000/140000 [==============================] - 0s 3us/step - loss: 0.5332 - acc: 0.7336 - sigLoss: -581.7840 - significance: 24.4128 - asimovSignificance: 17.6739 - truePositive: 0.6407 - falsePositive: 0.1736 - val_loss: 0.5329 - val_acc: 0.7316 - val_sigLoss: -584.0661 - val_significance: 24.4020 - val_asimovSignificance: 17.5039 - val_truePositive: 0.6404 - val_falsePositive: 0.1772\n",
      "Epoch 120/200\n",
      "140000/140000 [==============================] - 0s 3us/step - loss: 0.5333 - acc: 0.7336 - sigLoss: -582.3864 - significance: 24.4025 - asimovSignificance: 17.6988 - truePositive: 0.6401 - falsePositive: 0.1728 - val_loss: 0.5329 - val_acc: 0.7317 - val_sigLoss: -579.5049 - val_significance: 24.3181 - val_asimovSignificance: 17.6624 - val_truePositive: 0.6358 - val_falsePositive: 0.1725\n",
      "Epoch 121/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140000/140000 [==============================] - 0s 3us/step - loss: 0.5334 - acc: 0.7340 - sigLoss: -581.6565 - significance: 24.4101 - asimovSignificance: 17.7329 - truePositive: 0.6405 - falsePositive: 0.1725 - val_loss: 0.5329 - val_acc: 0.7314 - val_sigLoss: -583.7761 - val_significance: 24.3897 - val_asimovSignificance: 17.5075 - val_truePositive: 0.6398 - val_falsePositive: 0.1770\n",
      "Epoch 122/200\n",
      "140000/140000 [==============================] - 0s 3us/step - loss: 0.5333 - acc: 0.7339 - sigLoss: -581.6454 - significance: 24.3859 - asimovSignificance: 17.7741 - truePositive: 0.6391 - falsePositive: 0.1710 - val_loss: 0.5328 - val_acc: 0.7315 - val_sigLoss: -581.2309 - val_significance: 24.3881 - val_asimovSignificance: 17.5140 - val_truePositive: 0.6397 - val_falsePositive: 0.1768\n",
      "Epoch 123/200\n",
      "140000/140000 [==============================] - 0s 3us/step - loss: 0.5329 - acc: 0.7339 - sigLoss: -582.3372 - significance: 24.4141 - asimovSignificance: 17.7105 - truePositive: 0.6408 - falsePositive: 0.1730 - val_loss: 0.5328 - val_acc: 0.7313 - val_sigLoss: -580.3796 - val_significance: 24.3286 - val_asimovSignificance: 17.6056 - val_truePositive: 0.6364 - val_falsePositive: 0.1739\n",
      "Epoch 124/200\n",
      "140000/140000 [==============================] - 0s 3us/step - loss: 0.5336 - acc: 0.7339 - sigLoss: -581.7903 - significance: 24.3868 - asimovSignificance: 17.7734 - truePositive: 0.6392 - falsePositive: 0.1713 - val_loss: 0.5329 - val_acc: 0.7314 - val_sigLoss: -583.5028 - val_significance: 24.4188 - val_asimovSignificance: 17.4580 - val_truePositive: 0.6414 - val_falsePositive: 0.1786\n",
      "Epoch 125/200\n",
      "140000/140000 [==============================] - 0s 3us/step - loss: 0.5334 - acc: 0.7336 - sigLoss: -581.8898 - significance: 24.4122 - asimovSignificance: 17.6925 - truePositive: 0.6406 - falsePositive: 0.1735 - val_loss: 0.5329 - val_acc: 0.7312 - val_sigLoss: -583.0875 - val_significance: 24.4288 - val_asimovSignificance: 17.4200 - val_truePositive: 0.6419 - val_falsePositive: 0.1796\n",
      "Epoch 126/200\n",
      "140000/140000 [==============================] - 0s 3us/step - loss: 0.5332 - acc: 0.7335 - sigLoss: -582.4015 - significance: 24.3919 - asimovSignificance: 17.7060 - truePositive: 0.6396 - falsePositive: 0.1727 - val_loss: 0.5328 - val_acc: 0.7314 - val_sigLoss: -580.3175 - val_significance: 24.3643 - val_asimovSignificance: 17.5475 - val_truePositive: 0.6384 - val_falsePositive: 0.1757\n",
      "Epoch 127/200\n",
      "140000/140000 [==============================] - 0s 3us/step - loss: 0.5336 - acc: 0.7340 - sigLoss: -581.6621 - significance: 24.4580 - asimovSignificance: 17.6344 - truePositive: 0.6432 - falsePositive: 0.1752 - val_loss: 0.5328 - val_acc: 0.7319 - val_sigLoss: -580.3641 - val_significance: 24.3259 - val_asimovSignificance: 17.6643 - val_truePositive: 0.6362 - val_falsePositive: 0.1725\n",
      "Epoch 128/200\n",
      "140000/140000 [==============================] - 0s 2us/step - loss: 0.5333 - acc: 0.7342 - sigLoss: -581.9323 - significance: 24.4029 - asimovSignificance: 17.7443 - truePositive: 0.6401 - falsePositive: 0.1719 - val_loss: 0.5329 - val_acc: 0.7310 - val_sigLoss: -581.2014 - val_significance: 24.3369 - val_asimovSignificance: 17.5680 - val_truePositive: 0.6369 - val_falsePositive: 0.1749\n",
      "Epoch 129/200\n",
      "140000/140000 [==============================] - 0s 3us/step - loss: 0.5333 - acc: 0.7341 - sigLoss: -582.6179 - significance: 24.4389 - asimovSignificance: 17.6923 - truePositive: 0.6421 - falsePositive: 0.1738 - val_loss: 0.5330 - val_acc: 0.7316 - val_sigLoss: -579.8032 - val_significance: 24.2501 - val_asimovSignificance: 17.7800 - val_truePositive: 0.6320 - val_falsePositive: 0.1690\n",
      "Epoch 130/200\n",
      "140000/140000 [==============================] - 0s 3us/step - loss: 0.5333 - acc: 0.7338 - sigLoss: -581.3387 - significance: 24.3726 - asimovSignificance: 17.7867 - truePositive: 0.6385 - falsePositive: 0.1708 - val_loss: 0.5329 - val_acc: 0.7313 - val_sigLoss: -579.4847 - val_significance: 24.2700 - val_asimovSignificance: 17.7190 - val_truePositive: 0.6332 - val_falsePositive: 0.1706\n",
      "Epoch 131/200\n",
      "140000/140000 [==============================] - 0s 3us/step - loss: 0.5333 - acc: 0.7339 - sigLoss: -581.4702 - significance: 24.3876 - asimovSignificance: 17.7729 - truePositive: 0.6393 - falsePositive: 0.1714 - val_loss: 0.5328 - val_acc: 0.7311 - val_sigLoss: -583.7956 - val_significance: 24.3624 - val_asimovSignificance: 17.5296 - val_truePositive: 0.6383 - val_falsePositive: 0.1761\n",
      "Epoch 132/200\n",
      "140000/140000 [==============================] - 0s 3us/step - loss: 0.5333 - acc: 0.7336 - sigLoss: -582.4211 - significance: 24.4129 - asimovSignificance: 17.6793 - truePositive: 0.6407 - falsePositive: 0.1734 - val_loss: 0.5329 - val_acc: 0.7317 - val_sigLoss: -579.3601 - val_significance: 24.2893 - val_asimovSignificance: 17.7203 - val_truePositive: 0.6342 - val_falsePositive: 0.1708\n",
      "Epoch 133/200\n",
      "140000/140000 [==============================] - 0s 3us/step - loss: 0.5336 - acc: 0.7339 - sigLoss: -581.4563 - significance: 24.3852 - asimovSignificance: 17.7711 - truePositive: 0.6391 - falsePositive: 0.1713 - val_loss: 0.5330 - val_acc: 0.7313 - val_sigLoss: -586.0853 - val_significance: 24.4650 - val_asimovSignificance: 17.3624 - val_truePositive: 0.6440 - val_falsePositive: 0.1815\n",
      "Epoch 134/200\n",
      "140000/140000 [==============================] - 0s 3us/step - loss: 0.5336 - acc: 0.7337 - sigLoss: -581.9689 - significance: 24.4214 - asimovSignificance: 17.6764 - truePositive: 0.6412 - falsePositive: 0.1738 - val_loss: 0.5328 - val_acc: 0.7317 - val_sigLoss: -582.3837 - val_significance: 24.3824 - val_asimovSignificance: 17.5427 - val_truePositive: 0.6393 - val_falsePositive: 0.1761\n",
      "Epoch 135/200\n",
      "140000/140000 [==============================] - 0s 3us/step - loss: 0.5335 - acc: 0.7337 - sigLoss: -581.6969 - significance: 24.4329 - asimovSignificance: 17.6644 - truePositive: 0.6418 - falsePositive: 0.1745 - val_loss: 0.5329 - val_acc: 0.7315 - val_sigLoss: -583.2972 - val_significance: 24.3847 - val_asimovSignificance: 17.5259 - val_truePositive: 0.6395 - val_falsePositive: 0.1765\n",
      "Epoch 136/200\n",
      "140000/140000 [==============================] - 0s 3us/step - loss: 0.5333 - acc: 0.7337 - sigLoss: -581.9954 - significance: 24.4110 - asimovSignificance: 17.7116 - truePositive: 0.6406 - falsePositive: 0.1730 - val_loss: 0.5329 - val_acc: 0.7312 - val_sigLoss: -580.3514 - val_significance: 24.3242 - val_asimovSignificance: 17.6048 - val_truePositive: 0.6362 - val_falsePositive: 0.1738\n",
      "Epoch 137/200\n",
      "140000/140000 [==============================] - 0s 3us/step - loss: 0.5334 - acc: 0.7334 - sigLoss: -582.0035 - significance: 24.3808 - asimovSignificance: 17.7310 - truePositive: 0.6389 - falsePositive: 0.1719 - val_loss: 0.5329 - val_acc: 0.7315 - val_sigLoss: -580.6350 - val_significance: 24.4028 - val_asimovSignificance: 17.4960 - val_truePositive: 0.6405 - val_falsePositive: 0.1775\n",
      "Epoch 138/200\n",
      "140000/140000 [==============================] - 0s 3us/step - loss: 0.5333 - acc: 0.7339 - sigLoss: -582.1082 - significance: 24.4569 - asimovSignificance: 17.6412 - truePositive: 0.6431 - falsePositive: 0.1752 - val_loss: 0.5330 - val_acc: 0.7313 - val_sigLoss: -583.6261 - val_significance: 24.3531 - val_asimovSignificance: 17.5612 - val_truePositive: 0.6378 - val_falsePositive: 0.1752\n",
      "Epoch 139/200\n",
      "140000/140000 [==============================] - 0s 3us/step - loss: 0.5333 - acc: 0.7339 - sigLoss: -582.2537 - significance: 24.4114 - asimovSignificance: 17.7152 - truePositive: 0.6406 - falsePositive: 0.1728 - val_loss: 0.5329 - val_acc: 0.7312 - val_sigLoss: -581.2586 - val_significance: 24.3486 - val_asimovSignificance: 17.5586 - val_truePositive: 0.6375 - val_falsePositive: 0.1752\n",
      "Epoch 140/200\n",
      "140000/140000 [==============================] - 0s 3us/step - loss: 0.5330 - acc: 0.7335 - sigLoss: -582.3070 - significance: 24.4347 - asimovSignificance: 17.6422 - truePositive: 0.6419 - falsePositive: 0.1750 - val_loss: 0.5329 - val_acc: 0.7312 - val_sigLoss: -581.2354 - val_significance: 24.3333 - val_asimovSignificance: 17.5937 - val_truePositive: 0.6367 - val_falsePositive: 0.1742\n",
      "Epoch 141/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140000/140000 [==============================] - 0s 3us/step - loss: 0.5334 - acc: 0.7339 - sigLoss: -581.5526 - significance: 24.4120 - asimovSignificance: 17.7186 - truePositive: 0.6406 - falsePositive: 0.1727 - val_loss: 0.5328 - val_acc: 0.7315 - val_sigLoss: -581.0905 - val_significance: 24.3434 - val_asimovSignificance: 17.5976 - val_truePositive: 0.6372 - val_falsePositive: 0.1743\n",
      "Epoch 142/200\n",
      "140000/140000 [==============================] - 0s 3us/step - loss: 0.5332 - acc: 0.7338 - sigLoss: -582.1904 - significance: 24.4016 - asimovSignificance: 17.7164 - truePositive: 0.6400 - falsePositive: 0.1724 - val_loss: 0.5328 - val_acc: 0.7313 - val_sigLoss: -580.6984 - val_significance: 24.3713 - val_asimovSignificance: 17.5319 - val_truePositive: 0.6387 - val_falsePositive: 0.1762\n",
      "Epoch 143/200\n",
      "140000/140000 [==============================] - 0s 3us/step - loss: 0.5333 - acc: 0.7331 - sigLoss: -581.8751 - significance: 24.4168 - asimovSignificance: 17.6345 - truePositive: 0.6410 - falsePositive: 0.1747 - val_loss: 0.5328 - val_acc: 0.7315 - val_sigLoss: -581.9998 - val_significance: 24.3350 - val_asimovSignificance: 17.6128 - val_truePositive: 0.6367 - val_falsePositive: 0.1739\n",
      "Epoch 144/200\n",
      "140000/140000 [==============================] - 0s 3us/step - loss: 0.5335 - acc: 0.7332 - sigLoss: -582.1732 - significance: 24.4374 - asimovSignificance: 17.6092 - truePositive: 0.6421 - falsePositive: 0.1755 - val_loss: 0.5329 - val_acc: 0.7317 - val_sigLoss: -579.3221 - val_significance: 24.3301 - val_asimovSignificance: 17.6383 - val_truePositive: 0.6364 - val_falsePositive: 0.1732\n",
      "Epoch 145/200\n",
      "140000/140000 [==============================] - 0s 3us/step - loss: 0.5338 - acc: 0.7337 - sigLoss: -580.8972 - significance: 24.4048 - asimovSignificance: 17.7100 - truePositive: 0.6402 - falsePositive: 0.1728 - val_loss: 0.5329 - val_acc: 0.7315 - val_sigLoss: -582.8059 - val_significance: 24.3197 - val_asimovSignificance: 17.6430 - val_truePositive: 0.6359 - val_falsePositive: 0.1729\n",
      "Epoch 146/200\n",
      "140000/140000 [==============================] - 0s 3us/step - loss: 0.5333 - acc: 0.7338 - sigLoss: -582.4717 - significance: 24.3997 - asimovSignificance: 17.7386 - truePositive: 0.6399 - falsePositive: 0.1722 - val_loss: 0.5329 - val_acc: 0.7316 - val_sigLoss: -579.8197 - val_significance: 24.3500 - val_asimovSignificance: 17.6011 - val_truePositive: 0.6375 - val_falsePositive: 0.1743\n",
      "Epoch 147/200\n",
      "140000/140000 [==============================] - 0s 2us/step - loss: 0.5329 - acc: 0.7337 - sigLoss: -581.9170 - significance: 24.4314 - asimovSignificance: 17.6725 - truePositive: 0.6417 - falsePositive: 0.1742 - val_loss: 0.5328 - val_acc: 0.7319 - val_sigLoss: -581.6606 - val_significance: 24.3309 - val_asimovSignificance: 17.6626 - val_truePositive: 0.6364 - val_falsePositive: 0.1727\n",
      "Epoch 148/200\n",
      "140000/140000 [==============================] - 0s 2us/step - loss: 0.5332 - acc: 0.7334 - sigLoss: -582.0308 - significance: 24.3968 - asimovSignificance: 17.7064 - truePositive: 0.6398 - falsePositive: 0.1729 - val_loss: 0.5329 - val_acc: 0.7317 - val_sigLoss: -581.5580 - val_significance: 24.3061 - val_asimovSignificance: 17.6887 - val_truePositive: 0.6351 - val_falsePositive: 0.1717\n",
      "Epoch 149/200\n",
      "140000/140000 [==============================] - 0s 3us/step - loss: 0.5338 - acc: 0.7338 - sigLoss: -582.5343 - significance: 24.4351 - asimovSignificance: 17.6667 - truePositive: 0.6419 - falsePositive: 0.1743 - val_loss: 0.5329 - val_acc: 0.7315 - val_sigLoss: -579.7491 - val_significance: 24.3298 - val_asimovSignificance: 17.6258 - val_truePositive: 0.6364 - val_falsePositive: 0.1734\n",
      "Epoch 150/200\n",
      "140000/140000 [==============================] - 0s 3us/step - loss: 0.5333 - acc: 0.7336 - sigLoss: -581.3864 - significance: 24.3939 - asimovSignificance: 17.7211 - truePositive: 0.6397 - falsePositive: 0.1725 - val_loss: 0.5329 - val_acc: 0.7315 - val_sigLoss: -581.1076 - val_significance: 24.3266 - val_asimovSignificance: 17.6343 - val_truePositive: 0.6362 - val_falsePositive: 0.1733\n",
      "Epoch 151/200\n",
      "140000/140000 [==============================] - 0s 3us/step - loss: 0.5330 - acc: 0.7339 - sigLoss: -581.4024 - significance: 24.4067 - asimovSignificance: 17.7240 - truePositive: 0.6403 - falsePositive: 0.1725 - val_loss: 0.5329 - val_acc: 0.7315 - val_sigLoss: -584.3519 - val_significance: 24.3755 - val_asimovSignificance: 17.5414 - val_truePositive: 0.6390 - val_falsePositive: 0.1760\n",
      "Epoch 152/200\n",
      "140000/140000 [==============================] - 0s 3us/step - loss: 0.5335 - acc: 0.7337 - sigLoss: -582.2015 - significance: 24.4129 - asimovSignificance: 17.6895 - truePositive: 0.6407 - falsePositive: 0.1734 - val_loss: 0.5328 - val_acc: 0.7314 - val_sigLoss: -580.3188 - val_significance: 24.3356 - val_asimovSignificance: 17.6102 - val_truePositive: 0.6368 - val_falsePositive: 0.1739\n",
      "Epoch 153/200\n",
      "140000/140000 [==============================] - 0s 3us/step - loss: 0.5330 - acc: 0.7340 - sigLoss: -581.7310 - significance: 24.4041 - asimovSignificance: 17.7568 - truePositive: 0.6402 - falsePositive: 0.1721 - val_loss: 0.5329 - val_acc: 0.7311 - val_sigLoss: -582.6506 - val_significance: 24.4200 - val_asimovSignificance: 17.4283 - val_truePositive: 0.6415 - val_falsePositive: 0.1793\n",
      "Epoch 154/200\n",
      "140000/140000 [==============================] - 0s 3us/step - loss: 0.5333 - acc: 0.7337 - sigLoss: -581.9859 - significance: 24.4316 - asimovSignificance: 17.6687 - truePositive: 0.6417 - falsePositive: 0.1741 - val_loss: 0.5328 - val_acc: 0.7316 - val_sigLoss: -582.3280 - val_significance: 24.3564 - val_asimovSignificance: 17.5836 - val_truePositive: 0.6379 - val_falsePositive: 0.1748\n",
      "Epoch 155/200\n",
      "140000/140000 [==============================] - 0s 3us/step - loss: 0.5337 - acc: 0.7338 - sigLoss: -582.6074 - significance: 24.4274 - asimovSignificance: 17.6894 - truePositive: 0.6415 - falsePositive: 0.1740 - val_loss: 0.5328 - val_acc: 0.7318 - val_sigLoss: -580.4977 - val_significance: 24.3558 - val_asimovSignificance: 17.6056 - val_truePositive: 0.6378 - val_falsePositive: 0.1742\n",
      "Epoch 156/200\n",
      "140000/140000 [==============================] - 0s 3us/step - loss: 0.5337 - acc: 0.7333 - sigLoss: -581.1467 - significance: 24.3730 - asimovSignificance: 17.7315 - truePositive: 0.6385 - falsePositive: 0.1719 - val_loss: 0.5328 - val_acc: 0.7311 - val_sigLoss: -580.6880 - val_significance: 24.3755 - val_asimovSignificance: 17.5020 - val_truePositive: 0.6390 - val_falsePositive: 0.1769\n",
      "Epoch 157/200\n",
      "140000/140000 [==============================] - 0s 3us/step - loss: 0.5332 - acc: 0.7336 - sigLoss: -581.7020 - significance: 24.4112 - asimovSignificance: 17.6928 - truePositive: 0.6406 - falsePositive: 0.1733 - val_loss: 0.5329 - val_acc: 0.7316 - val_sigLoss: -583.4170 - val_significance: 24.3775 - val_asimovSignificance: 17.5487 - val_truePositive: 0.6390 - val_falsePositive: 0.1759\n",
      "Epoch 158/200\n",
      "140000/140000 [==============================] - 0s 3us/step - loss: 0.5329 - acc: 0.7333 - sigLoss: -582.1749 - significance: 24.4039 - asimovSignificance: 17.6803 - truePositive: 0.6403 - falsePositive: 0.1736 - val_loss: 0.5329 - val_acc: 0.7312 - val_sigLoss: -584.6856 - val_significance: 24.4222 - val_asimovSignificance: 17.4289 - val_truePositive: 0.6416 - val_falsePositive: 0.1793\n",
      "Epoch 159/200\n",
      "140000/140000 [==============================] - 0s 3us/step - loss: 0.5334 - acc: 0.7336 - sigLoss: -581.8059 - significance: 24.4107 - asimovSignificance: 17.6932 - truePositive: 0.6406 - falsePositive: 0.1733 - val_loss: 0.5329 - val_acc: 0.7316 - val_sigLoss: -583.6257 - val_significance: 24.3960 - val_asimovSignificance: 17.5114 - val_truePositive: 0.6401 - val_falsePositive: 0.1770\n",
      "Epoch 160/200\n",
      "140000/140000 [==============================] - 0s 3us/step - loss: 0.5333 - acc: 0.7334 - sigLoss: -582.0922 - significance: 24.4042 - asimovSignificance: 17.6855 - truePositive: 0.6403 - falsePositive: 0.1733 - val_loss: 0.5328 - val_acc: 0.7312 - val_sigLoss: -580.1471 - val_significance: 24.3356 - val_asimovSignificance: 17.5905 - val_truePositive: 0.6368 - val_falsePositive: 0.1744\n",
      "Epoch 161/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140000/140000 [==============================] - 0s 3us/step - loss: 0.5331 - acc: 0.7335 - sigLoss: -581.6127 - significance: 24.3852 - asimovSignificance: 17.7392 - truePositive: 0.6392 - falsePositive: 0.1722 - val_loss: 0.5329 - val_acc: 0.7317 - val_sigLoss: -582.9074 - val_significance: 24.3712 - val_asimovSignificance: 17.5678 - val_truePositive: 0.6387 - val_falsePositive: 0.1754\n",
      "Epoch 162/200\n",
      "140000/140000 [==============================] - 0s 3us/step - loss: 0.5332 - acc: 0.7340 - sigLoss: -582.3703 - significance: 24.4207 - asimovSignificance: 17.7036 - truePositive: 0.6411 - falsePositive: 0.1732 - val_loss: 0.5328 - val_acc: 0.7318 - val_sigLoss: -580.3877 - val_significance: 24.3445 - val_asimovSignificance: 17.6255 - val_truePositive: 0.6372 - val_falsePositive: 0.1737\n",
      "Epoch 163/200\n",
      "140000/140000 [==============================] - 0s 3us/step - loss: 0.5333 - acc: 0.7334 - sigLoss: -581.5805 - significance: 24.3777 - asimovSignificance: 17.7366 - truePositive: 0.6388 - falsePositive: 0.1718 - val_loss: 0.5328 - val_acc: 0.7312 - val_sigLoss: -583.1559 - val_significance: 24.3790 - val_asimovSignificance: 17.5088 - val_truePositive: 0.6392 - val_falsePositive: 0.1768\n",
      "Epoch 164/200\n",
      "140000/140000 [==============================] - 0s 3us/step - loss: 0.5335 - acc: 0.7336 - sigLoss: -582.2798 - significance: 24.4494 - asimovSignificance: 17.6052 - truePositive: 0.6428 - falsePositive: 0.1757 - val_loss: 0.5328 - val_acc: 0.7317 - val_sigLoss: -580.3783 - val_significance: 24.2893 - val_asimovSignificance: 17.7210 - val_truePositive: 0.6342 - val_falsePositive: 0.1708\n",
      "Epoch 165/200\n",
      "140000/140000 [==============================] - 0s 3us/step - loss: 0.5334 - acc: 0.7338 - sigLoss: -581.6731 - significance: 24.4121 - asimovSignificance: 17.7114 - truePositive: 0.6406 - falsePositive: 0.1730 - val_loss: 0.5329 - val_acc: 0.7312 - val_sigLoss: -584.2161 - val_significance: 24.3564 - val_asimovSignificance: 17.5424 - val_truePositive: 0.6380 - val_falsePositive: 0.1757\n",
      "Epoch 166/200\n",
      "140000/140000 [==============================] - 0s 3us/step - loss: 0.5335 - acc: 0.7333 - sigLoss: -581.9757 - significance: 24.4253 - asimovSignificance: 17.6419 - truePositive: 0.6414 - falsePositive: 0.1747 - val_loss: 0.5329 - val_acc: 0.7315 - val_sigLoss: -582.8195 - val_significance: 24.3490 - val_asimovSignificance: 17.5875 - val_truePositive: 0.6375 - val_falsePositive: 0.1746\n",
      "Epoch 167/200\n",
      "140000/140000 [==============================] - 0s 3us/step - loss: 0.5330 - acc: 0.7329 - sigLoss: -581.9334 - significance: 24.3851 - asimovSignificance: 17.6760 - truePositive: 0.6392 - falsePositive: 0.1734 - val_loss: 0.5328 - val_acc: 0.7316 - val_sigLoss: -581.6152 - val_significance: 24.3295 - val_asimovSignificance: 17.6336 - val_truePositive: 0.6364 - val_falsePositive: 0.1733\n",
      "Epoch 168/200\n",
      "140000/140000 [==============================] - 0s 3us/step - loss: 0.5331 - acc: 0.7335 - sigLoss: -581.9833 - significance: 24.3986 - asimovSignificance: 17.6942 - truePositive: 0.6400 - falsePositive: 0.1731 - val_loss: 0.5329 - val_acc: 0.7315 - val_sigLoss: -580.4554 - val_significance: 24.3170 - val_asimovSignificance: 17.6462 - val_truePositive: 0.6357 - val_falsePositive: 0.1729\n",
      "Epoch 169/200\n",
      "140000/140000 [==============================] - 0s 3us/step - loss: 0.5335 - acc: 0.7336 - sigLoss: -582.1907 - significance: 24.3961 - asimovSignificance: 17.7259 - truePositive: 0.6398 - falsePositive: 0.1725 - val_loss: 0.5328 - val_acc: 0.7310 - val_sigLoss: -579.3130 - val_significance: 24.3650 - val_asimovSignificance: 17.5156 - val_truePositive: 0.6384 - val_falsePositive: 0.1765\n",
      "Epoch 170/200\n",
      "140000/140000 [==============================] - 0s 3us/step - loss: 0.5329 - acc: 0.7334 - sigLoss: -581.6472 - significance: 24.4028 - asimovSignificance: 17.6790 - truePositive: 0.6402 - falsePositive: 0.1736 - val_loss: 0.5328 - val_acc: 0.7313 - val_sigLoss: -579.3787 - val_significance: 24.3171 - val_asimovSignificance: 17.6309 - val_truePositive: 0.6358 - val_falsePositive: 0.1732\n",
      "Epoch 171/200\n",
      "140000/140000 [==============================] - 0s 3us/step - loss: 0.5331 - acc: 0.7334 - sigLoss: -582.3002 - significance: 24.4298 - asimovSignificance: 17.6474 - truePositive: 0.6416 - falsePositive: 0.1747 - val_loss: 0.5328 - val_acc: 0.7312 - val_sigLoss: -579.6334 - val_significance: 24.3255 - val_asimovSignificance: 17.6070 - val_truePositive: 0.6362 - val_falsePositive: 0.1739\n",
      "Epoch 172/200\n",
      "140000/140000 [==============================] - 0s 3us/step - loss: 0.5332 - acc: 0.7338 - sigLoss: -581.2678 - significance: 24.4233 - asimovSignificance: 17.6731 - truePositive: 0.6413 - falsePositive: 0.1737 - val_loss: 0.5329 - val_acc: 0.7316 - val_sigLoss: -582.9956 - val_significance: 24.3421 - val_asimovSignificance: 17.6097 - val_truePositive: 0.6371 - val_falsePositive: 0.1740\n",
      "Epoch 173/200\n",
      "140000/140000 [==============================] - 0s 2us/step - loss: 0.5333 - acc: 0.7335 - sigLoss: -582.8704 - significance: 24.4177 - asimovSignificance: 17.6647 - truePositive: 0.6410 - falsePositive: 0.1740 - val_loss: 0.5329 - val_acc: 0.7313 - val_sigLoss: -578.9282 - val_significance: 24.3449 - val_asimovSignificance: 17.5826 - val_truePositive: 0.6373 - val_falsePositive: 0.1747\n",
      "Epoch 174/200\n",
      "140000/140000 [==============================] - 0s 2us/step - loss: 0.5334 - acc: 0.7340 - sigLoss: -581.2426 - significance: 24.4207 - asimovSignificance: 17.7129 - truePositive: 0.6410 - falsePositive: 0.1730 - val_loss: 0.5329 - val_acc: 0.7310 - val_sigLoss: -584.0522 - val_significance: 24.3902 - val_asimovSignificance: 17.4676 - val_truePositive: 0.6398 - val_falsePositive: 0.1779\n",
      "Epoch 175/200\n",
      "140000/140000 [==============================] - 0s 2us/step - loss: 0.5334 - acc: 0.7336 - sigLoss: -582.4223 - significance: 24.4261 - asimovSignificance: 17.6730 - truePositive: 0.6414 - falsePositive: 0.1742 - val_loss: 0.5328 - val_acc: 0.7314 - val_sigLoss: -578.9144 - val_significance: 24.3350 - val_asimovSignificance: 17.6086 - val_truePositive: 0.6367 - val_falsePositive: 0.1740\n",
      "Epoch 176/200\n",
      "140000/140000 [==============================] - 0s 3us/step - loss: 0.5333 - acc: 0.7338 - sigLoss: -580.9555 - significance: 24.3867 - asimovSignificance: 17.7566 - truePositive: 0.6392 - falsePositive: 0.1716 - val_loss: 0.5328 - val_acc: 0.7313 - val_sigLoss: -583.2295 - val_significance: 24.3596 - val_asimovSignificance: 17.5563 - val_truePositive: 0.6381 - val_falsePositive: 0.1755\n",
      "Epoch 177/200\n",
      "140000/140000 [==============================] - 0s 3us/step - loss: 0.5331 - acc: 0.7338 - sigLoss: -582.1617 - significance: 24.4282 - asimovSignificance: 17.6788 - truePositive: 0.6415 - falsePositive: 0.1737 - val_loss: 0.5330 - val_acc: 0.7313 - val_sigLoss: -585.6663 - val_significance: 24.3508 - val_asimovSignificance: 17.5665 - val_truePositive: 0.6376 - val_falsePositive: 0.1751\n",
      "Epoch 178/200\n",
      "140000/140000 [==============================] - 0s 3us/step - loss: 0.5332 - acc: 0.7339 - sigLoss: -582.7862 - significance: 24.4167 - asimovSignificance: 17.7075 - truePositive: 0.6409 - falsePositive: 0.1732 - val_loss: 0.5329 - val_acc: 0.7312 - val_sigLoss: -580.0673 - val_significance: 24.3278 - val_asimovSignificance: 17.6015 - val_truePositive: 0.6364 - val_falsePositive: 0.1740\n",
      "Epoch 179/200\n",
      "140000/140000 [==============================] - 0s 3us/step - loss: 0.5328 - acc: 0.7335 - sigLoss: -581.7143 - significance: 24.4136 - asimovSignificance: 17.6701 - truePositive: 0.6407 - falsePositive: 0.1736 - val_loss: 0.5328 - val_acc: 0.7312 - val_sigLoss: -583.1065 - val_significance: 24.3681 - val_asimovSignificance: 17.5307 - val_truePositive: 0.6386 - val_falsePositive: 0.1762\n",
      "Epoch 180/200\n",
      "140000/140000 [==============================] - 0s 3us/step - loss: 0.5332 - acc: 0.7335 - sigLoss: -581.7012 - significance: 24.4053 - asimovSignificance: 17.6823 - truePositive: 0.6403 - falsePositive: 0.1734 - val_loss: 0.5329 - val_acc: 0.7312 - val_sigLoss: -583.7678 - val_significance: 24.3653 - val_asimovSignificance: 17.5281 - val_truePositive: 0.6384 - val_falsePositive: 0.1762\n",
      "Epoch 181/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140000/140000 [==============================] - 0s 3us/step - loss: 0.5331 - acc: 0.7335 - sigLoss: -582.3897 - significance: 24.4097 - asimovSignificance: 17.6896 - truePositive: 0.6405 - falsePositive: 0.1735 - val_loss: 0.5330 - val_acc: 0.7314 - val_sigLoss: -585.3798 - val_significance: 24.4079 - val_asimovSignificance: 17.4724 - val_truePositive: 0.6408 - val_falsePositive: 0.1781\n",
      "Epoch 182/200\n",
      "140000/140000 [==============================] - 0s 2us/step - loss: 0.5335 - acc: 0.7337 - sigLoss: -581.7974 - significance: 24.3877 - asimovSignificance: 17.7363 - truePositive: 0.6393 - falsePositive: 0.1720 - val_loss: 0.5328 - val_acc: 0.7312 - val_sigLoss: -583.1152 - val_significance: 24.3742 - val_asimovSignificance: 17.5164 - val_truePositive: 0.6389 - val_falsePositive: 0.1765\n",
      "Epoch 183/200\n",
      "140000/140000 [==============================] - 0s 2us/step - loss: 0.5333 - acc: 0.7335 - sigLoss: -581.9711 - significance: 24.4039 - asimovSignificance: 17.6878 - truePositive: 0.6402 - falsePositive: 0.1732 - val_loss: 0.5329 - val_acc: 0.7314 - val_sigLoss: -583.5546 - val_significance: 24.3446 - val_asimovSignificance: 17.5890 - val_truePositive: 0.6373 - val_falsePositive: 0.1745\n",
      "Epoch 184/200\n",
      "140000/140000 [==============================] - 0s 2us/step - loss: 0.5334 - acc: 0.7335 - sigLoss: -582.5082 - significance: 24.4294 - asimovSignificance: 17.6638 - truePositive: 0.6416 - falsePositive: 0.1745 - val_loss: 0.5329 - val_acc: 0.7317 - val_sigLoss: -583.0639 - val_significance: 24.3261 - val_asimovSignificance: 17.6564 - val_truePositive: 0.6362 - val_falsePositive: 0.1728\n",
      "Epoch 185/200\n",
      "140000/140000 [==============================] - 0s 2us/step - loss: 0.5335 - acc: 0.7334 - sigLoss: -581.8792 - significance: 24.3517 - asimovSignificance: 17.7770 - truePositive: 0.6374 - falsePositive: 0.1705 - val_loss: 0.5329 - val_acc: 0.7315 - val_sigLoss: -581.4473 - val_significance: 24.3521 - val_asimovSignificance: 17.5863 - val_truePositive: 0.6377 - val_falsePositive: 0.1747\n",
      "Epoch 186/200\n",
      "140000/140000 [==============================] - 0s 2us/step - loss: 0.5334 - acc: 0.7337 - sigLoss: -581.4413 - significance: 24.4352 - asimovSignificance: 17.6561 - truePositive: 0.6420 - falsePositive: 0.1744 - val_loss: 0.5328 - val_acc: 0.7317 - val_sigLoss: -579.1440 - val_significance: 24.2879 - val_asimovSignificance: 17.7190 - val_truePositive: 0.6341 - val_falsePositive: 0.1708\n",
      "Epoch 187/200\n",
      "140000/140000 [==============================] - 0s 2us/step - loss: 0.5332 - acc: 0.7332 - sigLoss: -582.1725 - significance: 24.3845 - asimovSignificance: 17.7096 - truePositive: 0.6391 - falsePositive: 0.1726 - val_loss: 0.5329 - val_acc: 0.7315 - val_sigLoss: -580.1287 - val_significance: 24.3291 - val_asimovSignificance: 17.6249 - val_truePositive: 0.6364 - val_falsePositive: 0.1735\n",
      "Epoch 188/200\n",
      "140000/140000 [==============================] - 0s 2us/step - loss: 0.5333 - acc: 0.7337 - sigLoss: -581.6565 - significance: 24.4153 - asimovSignificance: 17.6812 - truePositive: 0.6408 - falsePositive: 0.1735 - val_loss: 0.5329 - val_acc: 0.7310 - val_sigLoss: -580.8941 - val_significance: 24.3392 - val_asimovSignificance: 17.5627 - val_truePositive: 0.6370 - val_falsePositive: 0.1751\n",
      "Epoch 189/200\n",
      "140000/140000 [==============================] - 0s 2us/step - loss: 0.5334 - acc: 0.7336 - sigLoss: -582.2828 - significance: 24.4057 - asimovSignificance: 17.6804 - truePositive: 0.6403 - falsePositive: 0.1732 - val_loss: 0.5329 - val_acc: 0.7316 - val_sigLoss: -578.8493 - val_significance: 24.3046 - val_asimovSignificance: 17.6822 - val_truePositive: 0.6350 - val_falsePositive: 0.1719\n",
      "Epoch 190/200\n",
      "140000/140000 [==============================] - 0s 2us/step - loss: 0.5335 - acc: 0.7340 - sigLoss: -581.4815 - significance: 24.4269 - asimovSignificance: 17.7012 - truePositive: 0.6414 - falsePositive: 0.1735 - val_loss: 0.5329 - val_acc: 0.7317 - val_sigLoss: -581.9652 - val_significance: 24.3455 - val_asimovSignificance: 17.6176 - val_truePositive: 0.6373 - val_falsePositive: 0.1739\n",
      "Epoch 191/200\n",
      "140000/140000 [==============================] - 0s 2us/step - loss: 0.5333 - acc: 0.7339 - sigLoss: -582.5448 - significance: 24.4083 - asimovSignificance: 17.7270 - truePositive: 0.6404 - falsePositive: 0.1725 - val_loss: 0.5329 - val_acc: 0.7314 - val_sigLoss: -580.3586 - val_significance: 24.3523 - val_asimovSignificance: 17.5687 - val_truePositive: 0.6377 - val_falsePositive: 0.1750\n",
      "Epoch 192/200\n",
      "140000/140000 [==============================] - 0s 2us/step - loss: 0.5329 - acc: 0.7339 - sigLoss: -581.9522 - significance: 24.4105 - asimovSignificance: 17.7100 - truePositive: 0.6405 - falsePositive: 0.1728 - val_loss: 0.5328 - val_acc: 0.7316 - val_sigLoss: -581.3105 - val_significance: 24.3880 - val_asimovSignificance: 17.5282 - val_truePositive: 0.6396 - val_falsePositive: 0.1765\n",
      "Epoch 193/200\n",
      "140000/140000 [==============================] - 0s 2us/step - loss: 0.5329 - acc: 0.7340 - sigLoss: -582.4590 - significance: 24.4547 - asimovSignificance: 17.6501 - truePositive: 0.6430 - falsePositive: 0.1750 - val_loss: 0.5329 - val_acc: 0.7316 - val_sigLoss: -579.1249 - val_significance: 24.3275 - val_asimovSignificance: 17.6345 - val_truePositive: 0.6363 - val_falsePositive: 0.1732\n",
      "Epoch 194/200\n",
      "140000/140000 [==============================] - 0s 3us/step - loss: 0.5339 - acc: 0.7337 - sigLoss: -580.6428 - significance: 24.3871 - asimovSignificance: 17.7464 - truePositive: 0.6393 - falsePositive: 0.1718 - val_loss: 0.5328 - val_acc: 0.7314 - val_sigLoss: -581.5882 - val_significance: 24.3819 - val_asimovSignificance: 17.5244 - val_truePositive: 0.6393 - val_falsePositive: 0.1765\n",
      "Epoch 195/200\n",
      "140000/140000 [==============================] - 0s 3us/step - loss: 0.5333 - acc: 0.7334 - sigLoss: -582.8825 - significance: 24.4077 - asimovSignificance: 17.6689 - truePositive: 0.6404 - falsePositive: 0.1736 - val_loss: 0.5329 - val_acc: 0.7319 - val_sigLoss: -577.6308 - val_significance: 24.3188 - val_asimovSignificance: 17.6804 - val_truePositive: 0.6358 - val_falsePositive: 0.1721\n",
      "Epoch 196/200\n",
      "140000/140000 [==============================] - 0s 2us/step - loss: 0.5332 - acc: 0.7335 - sigLoss: -581.2475 - significance: 24.3893 - asimovSignificance: 17.7248 - truePositive: 0.6395 - falsePositive: 0.1725 - val_loss: 0.5328 - val_acc: 0.7312 - val_sigLoss: -582.1476 - val_significance: 24.3736 - val_asimovSignificance: 17.5220 - val_truePositive: 0.6389 - val_falsePositive: 0.1765\n",
      "Epoch 197/200\n",
      "140000/140000 [==============================] - 0s 2us/step - loss: 0.5333 - acc: 0.7337 - sigLoss: -581.9518 - significance: 24.4367 - asimovSignificance: 17.6494 - truePositive: 0.6420 - falsePositive: 0.1746 - val_loss: 0.5328 - val_acc: 0.7313 - val_sigLoss: -583.9720 - val_significance: 24.4160 - val_asimovSignificance: 17.4518 - val_truePositive: 0.6412 - val_falsePositive: 0.1786\n",
      "Epoch 198/200\n",
      "140000/140000 [==============================] - 0s 2us/step - loss: 0.5334 - acc: 0.7336 - sigLoss: -581.8321 - significance: 24.4285 - asimovSignificance: 17.6573 - truePositive: 0.6416 - falsePositive: 0.1744 - val_loss: 0.5328 - val_acc: 0.7311 - val_sigLoss: -580.8731 - val_significance: 24.3811 - val_asimovSignificance: 17.4941 - val_truePositive: 0.6393 - val_falsePositive: 0.1772\n",
      "Epoch 199/200\n",
      "140000/140000 [==============================] - 0s 2us/step - loss: 0.5334 - acc: 0.7334 - sigLoss: -582.0613 - significance: 24.4039 - asimovSignificance: 17.6749 - truePositive: 0.6403 - falsePositive: 0.1736 - val_loss: 0.5329 - val_acc: 0.7314 - val_sigLoss: -579.8320 - val_significance: 24.4028 - val_asimovSignificance: 17.4853 - val_truePositive: 0.6405 - val_falsePositive: 0.1777\n",
      "Epoch 200/200\n",
      "140000/140000 [==============================] - 0s 3us/step - loss: 0.5331 - acc: 0.7332 - sigLoss: -581.7008 - significance: 24.4489 - asimovSignificance: 17.5736 - truePositive: 0.6428 - falsePositive: 0.1765 - val_loss: 0.5329 - val_acc: 0.7318 - val_sigLoss: -583.3369 - val_significance: 24.3504 - val_asimovSignificance: 17.6128 - val_truePositive: 0.6375 - val_falsePositive: 0.1741\n",
      "22912/60000 [==========>...................] - ETA: 0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/felipe/hepML/MlClasses/Dnn.py:221: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  report = self.model.evaluate(X_test.as_matrix(), y_test.as_matrix(), sample_weight=weights_test, batch_size=batchSize)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 0s 7us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/felipe/hepML/MlClasses/Dnn.py:227: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  classificationReport(self.model.predict_classes(X_test.as_matrix()),self.model.predict(X_test.as_matrix()),y_test,f)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 23424/140000 [====>.........................] - ETA: 0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/felipe/hepML/MlClasses/Dnn.py:232: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  report = self.model.evaluate(X_train.as_matrix(), y_train.as_matrix(), sample_weight=weights_train, batch_size=batchSize)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140000/140000 [==============================] - 1s 6us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/felipe/hepML/MlClasses/Dnn.py:236: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  classificationReport(self.model.predict_classes(X_train.as_matrix()),self.model.predict(X_train.as_matrix()),y_train,f)\n",
      "/home/felipe/hepML/MlClasses/Dnn.py:254: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  rocCurve(self.model.predict(self.data.X_test.as_matrix()), self.data.y_test,self.output)\n",
      "/home/felipe/hepML/MlClasses/Dnn.py:255: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  rocCurve(self.model.predict(self.data.X_train.as_matrix()),self.data.y_train,self.output,append='_train')\n",
      "/home/felipe/hepML/MlClasses/Dnn.py:263: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  compareTrainTest(self.model.predict,self.data.X_train.as_matrix(),self.data.y_train.as_matrix(),\\\n",
      "/home/felipe/hepML/MlClasses/Dnn.py:264: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  self.data.X_test.as_matrix(),self.data.y_test.as_matrix(),self.output)\n",
      "/home/felipe/anaconda3/envs/tf-gpu/lib/python3.6/site-packages/matplotlib/axes/_axes.py:6521: MatplotlibDeprecationWarning: \n",
      "The 'normed' kwarg was deprecated in Matplotlib 2.1 and will be removed in 3.1. Use 'density' instead.\n",
      "  alternative=\"'density'\", removal=\"3.1\")\n",
      "/home/felipe/hepML/MlClasses/PerformanceTests.py:79: VisibleDeprecationWarning: Passing `normed=True` on non-uniform bins has always been broken, and computes neither the probability density function nor the probability mass function. The result is only correct if the bins are uniform, when density=True will produce the same result anyway. The argument will be removed in a future version of numpy.\n",
      "  bins=bins, range=low_high, normed=True)\n",
      "/home/felipe/hepML/MlClasses/PerformanceTests.py:88: VisibleDeprecationWarning: Passing `normed=True` on non-uniform bins has always been broken, and computes neither the probability density function nor the probability mass function. The result is only correct if the bins are uniform, when density=True will produce the same result anyway. The argument will be removed in a future version of numpy.\n",
      "  bins=bins, range=low_high, normed=True)\n",
      "/home/felipe/hepML/MlClasses/Dnn.py:368: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  return self.model.predict(self.data.X_test.as_matrix())\n",
      "/home/felipe/hepML/MlClasses/Dnn.py:405: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  dataTest['truth']=self.data.y_test.as_matrix()\n",
      "/home/felipe/anaconda3/envs/tf-gpu/lib/python3.6/site-packages/ipykernel_launcher.py:42: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "/home/felipe/anaconda3/envs/tf-gpu/lib/python3.6/site-packages/ipykernel_launcher.py:56: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "/home/felipe/hepML/MlClasses/Dnn.py:101: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  self.history = self.model.fit(self.data.X_train.as_matrix(), self.data.y_train.as_matrix(), sample_weight=self.data.weights_train,\n",
      "/home/felipe/hepML/MlClasses/Dnn.py:102: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  validation_data=(self.data.X_test.as_matrix(),self.data.y_test.as_matrix(),self.data.weights_test),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 57059 samples, validate on 24455 samples\n",
      "Epoch 1/200\n",
      "57059/57059 [==============================] - 0s 8us/step - loss: -768.7345 - acc: 0.5817 - sigLoss: -488.2132 - significance: 22.4450 - asimovSignificance: 12.0158 - truePositive: 0.5581 - falsePositive: 0.3331 - val_loss: -873.6285 - val_acc: 0.6794 - val_sigLoss: -557.1417 - val_significance: 25.7511 - val_asimovSignificance: 10.8260 - val_truePositive: 0.7364 - val_falsePositive: 0.5326\n",
      "Epoch 2/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -945.1717 - acc: 0.7306 - sigLoss: -600.3244 - significance: 27.8432 - asimovSignificance: 9.6011 - truePositive: 0.8741 - falsePositive: 0.7982 - val_loss: -1023.6332 - val_acc: 0.7777 - val_sigLoss: -652.8090 - val_significance: 29.3882 - val_asimovSignificance: 9.1581 - val_truePositive: 0.9771 - val_falsePositive: 0.9644\n",
      "Epoch 3/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1073.8551 - acc: 0.7788 - sigLoss: -681.8108 - significance: 29.5204 - asimovSignificance: 9.1126 - truePositive: 0.9865 - falsePositive: 0.9826 - val_loss: -1135.1698 - val_acc: 0.7878 - val_sigLoss: -723.9421 - val_significance: 29.7078 - val_asimovSignificance: 9.0908 - val_truePositive: 0.9994 - val_falsePositive: 0.9998\n",
      "Epoch 4/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1167.3591 - acc: 0.7850 - sigLoss: -741.2391 - significance: 29.7027 - asimovSignificance: 9.0910 - truePositive: 0.9990 - falsePositive: 0.9994 - val_loss: -1214.4477 - val_acc: 0.7882 - val_sigLoss: -774.5006 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 5/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1232.4273 - acc: 0.7856 - sigLoss: -782.5267 - significance: 29.7157 - asimovSignificance: 9.0925 - truePositive: 0.9999 - falsePositive: 1.0000 - val_loss: -1267.9829 - val_acc: 0.7882 - val_sigLoss: -808.6409 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 6/200\n",
      "57059/57059 [==============================] - 0s 3us/step - loss: -1274.9376 - acc: 0.7856 - sigLoss: -809.4855 - significance: 29.7168 - asimovSignificance: 9.0929 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1302.9855 - val_acc: 0.7882 - val_sigLoss: -830.9621 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 7/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1303.2951 - acc: 0.7856 - sigLoss: -827.5165 - significance: 29.7171 - asimovSignificance: 9.0930 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1325.8827 - val_acc: 0.7882 - val_sigLoss: -845.5637 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 8/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1321.5762 - acc: 0.7857 - sigLoss: -839.1237 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1341.1134 - val_acc: 0.7882 - val_sigLoss: -855.2763 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 9/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1335.8658 - acc: 0.7857 - sigLoss: -848.2120 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1351.4362 - val_acc: 0.7882 - val_sigLoss: -861.8594 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 10/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1344.7901 - acc: 0.7857 - sigLoss: -853.8826 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1358.6854 - val_acc: 0.7882 - val_sigLoss: -866.4823 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 11/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1352.2118 - acc: 0.7857 - sigLoss: -858.5864 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1363.9115 - val_acc: 0.7882 - val_sigLoss: -869.8152 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 12/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1357.0894 - acc: 0.7857 - sigLoss: -861.6949 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1367.8228 - val_acc: 0.7882 - val_sigLoss: -872.3096 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 13/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1361.3874 - acc: 0.7857 - sigLoss: -864.4167 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1370.8096 - val_acc: 0.7882 - val_sigLoss: -874.2144 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 14/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1365.1754 - acc: 0.7857 - sigLoss: -866.8209 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1373.1204 - val_acc: 0.7882 - val_sigLoss: -875.6881 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 15/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1367.4893 - acc: 0.7857 - sigLoss: -868.2864 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1374.9371 - val_acc: 0.7882 - val_sigLoss: -876.8467 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 16/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1369.8177 - acc: 0.7857 - sigLoss: -869.7679 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1376.4136 - val_acc: 0.7882 - val_sigLoss: -877.7883 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 17/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1372.0015 - acc: 0.7857 - sigLoss: -871.1595 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1377.5940 - val_acc: 0.7882 - val_sigLoss: -878.5412 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 18/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1373.7410 - acc: 0.7857 - sigLoss: -872.2580 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1378.5662 - val_acc: 0.7882 - val_sigLoss: -879.1612 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 19/200\n",
      "57059/57059 [==============================] - 0s 3us/step - loss: -1374.9667 - acc: 0.7857 - sigLoss: -873.0394 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1379.3796 - val_acc: 0.7882 - val_sigLoss: -879.6799 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 20/200\n",
      "57059/57059 [==============================] - 0s 3us/step - loss: -1375.9899 - acc: 0.7857 - sigLoss: -873.6853 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1380.0581 - val_acc: 0.7882 - val_sigLoss: -880.1127 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/200\n",
      "57059/57059 [==============================] - 0s 3us/step - loss: -1377.5974 - acc: 0.7857 - sigLoss: -874.7124 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1380.6327 - val_acc: 0.7882 - val_sigLoss: -880.4791 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 22/200\n",
      "57059/57059 [==============================] - 0s 3us/step - loss: -1378.4335 - acc: 0.7857 - sigLoss: -875.2336 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1381.1137 - val_acc: 0.7882 - val_sigLoss: -880.7859 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 23/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1379.2812 - acc: 0.7857 - sigLoss: -875.7767 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1381.5291 - val_acc: 0.7882 - val_sigLoss: -881.0508 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 24/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1379.8969 - acc: 0.7857 - sigLoss: -876.1694 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1381.8950 - val_acc: 0.7882 - val_sigLoss: -881.2841 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 25/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1380.4215 - acc: 0.7857 - sigLoss: -876.5009 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1382.2083 - val_acc: 0.7882 - val_sigLoss: -881.4840 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 26/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1381.3058 - acc: 0.7857 - sigLoss: -877.0677 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1382.4783 - val_acc: 0.7882 - val_sigLoss: -881.6562 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 27/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1381.4519 - acc: 0.7857 - sigLoss: -877.1534 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1382.7169 - val_acc: 0.7882 - val_sigLoss: -881.8083 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 28/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1381.7691 - acc: 0.7857 - sigLoss: -877.3628 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1382.9281 - val_acc: 0.7882 - val_sigLoss: -881.9430 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 29/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1382.5736 - acc: 0.7857 - sigLoss: -877.8694 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1383.1118 - val_acc: 0.7882 - val_sigLoss: -882.0602 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 30/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1382.8696 - acc: 0.7857 - sigLoss: -878.0575 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1383.2735 - val_acc: 0.7882 - val_sigLoss: -882.1633 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 31/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1383.3936 - acc: 0.7857 - sigLoss: -878.3881 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1383.4180 - val_acc: 0.7882 - val_sigLoss: -882.2555 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 32/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1384.1357 - acc: 0.7857 - sigLoss: -878.8645 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1383.5449 - val_acc: 0.7882 - val_sigLoss: -882.3364 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 33/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1383.9813 - acc: 0.7857 - sigLoss: -878.7621 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1383.6591 - val_acc: 0.7882 - val_sigLoss: -882.4093 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 34/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1384.2707 - acc: 0.7857 - sigLoss: -878.9478 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1383.7596 - val_acc: 0.7882 - val_sigLoss: -882.4733 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 35/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1384.4445 - acc: 0.7857 - sigLoss: -879.0537 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1383.8516 - val_acc: 0.7882 - val_sigLoss: -882.5320 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 36/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1384.6204 - acc: 0.7857 - sigLoss: -879.1752 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1383.9349 - val_acc: 0.7882 - val_sigLoss: -882.5851 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 37/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1385.2677 - acc: 0.7857 - sigLoss: -879.5775 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1384.0075 - val_acc: 0.7882 - val_sigLoss: -882.6314 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 38/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1385.2931 - acc: 0.7857 - sigLoss: -879.5960 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1384.0737 - val_acc: 0.7882 - val_sigLoss: -882.6737 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 39/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1385.4284 - acc: 0.7857 - sigLoss: -879.6813 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1384.1330 - val_acc: 0.7882 - val_sigLoss: -882.7115 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 40/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1385.9402 - acc: 0.7857 - sigLoss: -880.0044 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1384.1873 - val_acc: 0.7882 - val_sigLoss: -882.7461 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 41/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57059/57059 [==============================] - 0s 2us/step - loss: -1385.9004 - acc: 0.7857 - sigLoss: -879.9819 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1384.2365 - val_acc: 0.7882 - val_sigLoss: -882.7775 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 42/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1385.9343 - acc: 0.7857 - sigLoss: -880.0036 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1384.2820 - val_acc: 0.7882 - val_sigLoss: -882.8065 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 43/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1386.0563 - acc: 0.7857 - sigLoss: -880.0853 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1384.3232 - val_acc: 0.7882 - val_sigLoss: -882.8328 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 44/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1386.5806 - acc: 0.7857 - sigLoss: -880.4088 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1384.3599 - val_acc: 0.7882 - val_sigLoss: -882.8562 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 45/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1386.5514 - acc: 0.7857 - sigLoss: -880.3937 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1384.3927 - val_acc: 0.7882 - val_sigLoss: -882.8771 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 46/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1386.7514 - acc: 0.7857 - sigLoss: -880.5210 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1384.4232 - val_acc: 0.7882 - val_sigLoss: -882.8966 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 47/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1386.9190 - acc: 0.7857 - sigLoss: -880.6284 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1384.4513 - val_acc: 0.7882 - val_sigLoss: -882.9145 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 48/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1387.0182 - acc: 0.7857 - sigLoss: -880.6902 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1384.4769 - val_acc: 0.7882 - val_sigLoss: -882.9309 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 49/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1387.0611 - acc: 0.7857 - sigLoss: -880.7199 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1384.5010 - val_acc: 0.7882 - val_sigLoss: -882.9462 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 50/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1387.2959 - acc: 0.7857 - sigLoss: -880.8674 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1384.5229 - val_acc: 0.7882 - val_sigLoss: -882.9602 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 51/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1387.6049 - acc: 0.7857 - sigLoss: -881.0641 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1384.5423 - val_acc: 0.7882 - val_sigLoss: -882.9725 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 52/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1387.4314 - acc: 0.7857 - sigLoss: -880.9516 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1384.5607 - val_acc: 0.7882 - val_sigLoss: -882.9843 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 53/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1387.5402 - acc: 0.7857 - sigLoss: -881.0217 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1384.5780 - val_acc: 0.7882 - val_sigLoss: -882.9953 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 54/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1387.7051 - acc: 0.7857 - sigLoss: -881.1304 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1384.5933 - val_acc: 0.7882 - val_sigLoss: -883.0051 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 55/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1387.6228 - acc: 0.7857 - sigLoss: -881.0750 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1384.6071 - val_acc: 0.7882 - val_sigLoss: -883.0139 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 56/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1387.7325 - acc: 0.7857 - sigLoss: -881.1455 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1384.6208 - val_acc: 0.7882 - val_sigLoss: -883.0226 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 57/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1387.7787 - acc: 0.7857 - sigLoss: -881.1712 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1384.6326 - val_acc: 0.7882 - val_sigLoss: -883.0301 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 58/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1388.0268 - acc: 0.7857 - sigLoss: -881.3336 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1384.6439 - val_acc: 0.7882 - val_sigLoss: -883.0374 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 59/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1387.9500 - acc: 0.7857 - sigLoss: -881.2845 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1384.6544 - val_acc: 0.7882 - val_sigLoss: -883.0440 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 60/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1388.3499 - acc: 0.7857 - sigLoss: -881.5351 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1384.6634 - val_acc: 0.7882 - val_sigLoss: -883.0498 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 61/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57059/57059 [==============================] - 0s 2us/step - loss: -1388.3462 - acc: 0.7857 - sigLoss: -881.5326 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1384.6720 - val_acc: 0.7882 - val_sigLoss: -883.0553 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 62/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1388.3760 - acc: 0.7857 - sigLoss: -881.5523 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1384.6799 - val_acc: 0.7882 - val_sigLoss: -883.0603 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 63/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1388.1911 - acc: 0.7857 - sigLoss: -881.4336 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1384.6872 - val_acc: 0.7882 - val_sigLoss: -883.0650 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 64/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1388.3628 - acc: 0.7857 - sigLoss: -881.5438 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1384.6942 - val_acc: 0.7882 - val_sigLoss: -883.0694 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 65/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1388.5503 - acc: 0.7857 - sigLoss: -881.6631 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1384.7006 - val_acc: 0.7882 - val_sigLoss: -883.0735 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 66/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1388.6990 - acc: 0.7857 - sigLoss: -881.7588 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1384.7065 - val_acc: 0.7882 - val_sigLoss: -883.0773 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 67/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1388.3997 - acc: 0.7857 - sigLoss: -881.5691 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1384.7122 - val_acc: 0.7882 - val_sigLoss: -883.0809 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 68/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1388.7222 - acc: 0.7857 - sigLoss: -881.7708 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1384.7173 - val_acc: 0.7882 - val_sigLoss: -883.0842 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 69/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1388.6426 - acc: 0.7857 - sigLoss: -881.7224 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1384.7222 - val_acc: 0.7882 - val_sigLoss: -883.0873 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 70/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1388.5369 - acc: 0.7857 - sigLoss: -881.6546 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1384.7266 - val_acc: 0.7882 - val_sigLoss: -883.0901 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 71/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1388.6569 - acc: 0.7857 - sigLoss: -881.7318 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1384.7309 - val_acc: 0.7882 - val_sigLoss: -883.0928 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 72/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1388.6081 - acc: 0.7857 - sigLoss: -881.6954 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1384.7349 - val_acc: 0.7882 - val_sigLoss: -883.0954 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 73/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1388.8977 - acc: 0.7857 - sigLoss: -881.8832 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1384.7386 - val_acc: 0.7882 - val_sigLoss: -883.0978 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 74/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1388.7413 - acc: 0.7857 - sigLoss: -881.7832 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1384.7418 - val_acc: 0.7882 - val_sigLoss: -883.0998 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 75/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1388.9561 - acc: 0.7857 - sigLoss: -881.9231 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1384.7450 - val_acc: 0.7882 - val_sigLoss: -883.1018 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 76/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1389.0799 - acc: 0.7857 - sigLoss: -882.0026 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1384.7478 - val_acc: 0.7882 - val_sigLoss: -883.1036 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 77/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1389.1095 - acc: 0.7857 - sigLoss: -882.0167 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1384.7502 - val_acc: 0.7882 - val_sigLoss: -883.1052 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 78/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1388.8405 - acc: 0.7857 - sigLoss: -881.8446 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1384.7527 - val_acc: 0.7882 - val_sigLoss: -883.1068 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 79/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1389.0071 - acc: 0.7857 - sigLoss: -881.9525 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1384.7551 - val_acc: 0.7882 - val_sigLoss: -883.1083 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 80/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1389.2541 - acc: 0.7857 - sigLoss: -882.1090 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1384.7573 - val_acc: 0.7882 - val_sigLoss: -883.1097 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 81/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57059/57059 [==============================] - 0s 2us/step - loss: -1389.2439 - acc: 0.7857 - sigLoss: -882.1030 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1384.7593 - val_acc: 0.7882 - val_sigLoss: -883.1109 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 82/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1389.4462 - acc: 0.7857 - sigLoss: -882.2301 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1384.7609 - val_acc: 0.7882 - val_sigLoss: -883.1120 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 83/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1389.2588 - acc: 0.7857 - sigLoss: -882.1137 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1384.7628 - val_acc: 0.7882 - val_sigLoss: -883.1132 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 84/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1389.1870 - acc: 0.7857 - sigLoss: -882.0689 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1384.7644 - val_acc: 0.7882 - val_sigLoss: -883.1142 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 85/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1389.2446 - acc: 0.7857 - sigLoss: -882.1041 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1384.7659 - val_acc: 0.7882 - val_sigLoss: -883.1152 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 86/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1389.4543 - acc: 0.7857 - sigLoss: -882.2372 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1384.7673 - val_acc: 0.7882 - val_sigLoss: -883.1160 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 87/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1389.4522 - acc: 0.7857 - sigLoss: -882.2361 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1384.7684 - val_acc: 0.7882 - val_sigLoss: -883.1168 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 88/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1389.6289 - acc: 0.7857 - sigLoss: -882.3480 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1384.7696 - val_acc: 0.7882 - val_sigLoss: -883.1176 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 89/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1389.5597 - acc: 0.7857 - sigLoss: -882.3037 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1384.7708 - val_acc: 0.7882 - val_sigLoss: -883.1183 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 90/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1389.5596 - acc: 0.7857 - sigLoss: -882.3054 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1384.7718 - val_acc: 0.7882 - val_sigLoss: -883.1189 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 91/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1389.5626 - acc: 0.7857 - sigLoss: -882.3039 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1384.7728 - val_acc: 0.7882 - val_sigLoss: -883.1196 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 92/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1389.6874 - acc: 0.7857 - sigLoss: -882.3858 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1384.7738 - val_acc: 0.7882 - val_sigLoss: -883.1202 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 93/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1389.5064 - acc: 0.7857 - sigLoss: -882.2707 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1384.7746 - val_acc: 0.7882 - val_sigLoss: -883.1207 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 94/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1389.7394 - acc: 0.7857 - sigLoss: -882.4182 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1384.7754 - val_acc: 0.7882 - val_sigLoss: -883.1212 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 95/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1389.7396 - acc: 0.7857 - sigLoss: -882.4173 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1384.7761 - val_acc: 0.7882 - val_sigLoss: -883.1217 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 96/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1389.8604 - acc: 0.7857 - sigLoss: -882.4952 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1384.7768 - val_acc: 0.7882 - val_sigLoss: -883.1221 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 97/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1389.8199 - acc: 0.7857 - sigLoss: -882.4706 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1384.7773 - val_acc: 0.7882 - val_sigLoss: -883.1224 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 98/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1389.9127 - acc: 0.7857 - sigLoss: -882.5311 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1384.7778 - val_acc: 0.7882 - val_sigLoss: -883.1228 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 99/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1389.9212 - acc: 0.7857 - sigLoss: -882.5324 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1384.7784 - val_acc: 0.7882 - val_sigLoss: -883.1231 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 100/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1389.8263 - acc: 0.7857 - sigLoss: -882.4753 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1384.7789 - val_acc: 0.7882 - val_sigLoss: -883.1234 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 101/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57059/57059 [==============================] - 0s 2us/step - loss: -1390.0465 - acc: 0.7857 - sigLoss: -882.6135 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1384.7793 - val_acc: 0.7882 - val_sigLoss: -883.1237 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 102/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1389.9509 - acc: 0.7857 - sigLoss: -882.5510 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1384.7798 - val_acc: 0.7882 - val_sigLoss: -883.1240 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 103/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1389.9718 - acc: 0.7857 - sigLoss: -882.5665 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1384.7801 - val_acc: 0.7882 - val_sigLoss: -883.1242 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 104/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1390.0511 - acc: 0.7857 - sigLoss: -882.6156 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1384.7804 - val_acc: 0.7882 - val_sigLoss: -883.1244 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 105/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1389.9810 - acc: 0.7857 - sigLoss: -882.5722 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1384.7808 - val_acc: 0.7882 - val_sigLoss: -883.1246 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 106/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1390.1304 - acc: 0.7857 - sigLoss: -882.6682 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1384.7811 - val_acc: 0.7882 - val_sigLoss: -883.1248 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 107/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1390.1211 - acc: 0.7857 - sigLoss: -882.6614 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1384.7813 - val_acc: 0.7882 - val_sigLoss: -883.1250 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 108/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1390.2281 - acc: 0.7857 - sigLoss: -882.7297 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1384.7815 - val_acc: 0.7882 - val_sigLoss: -883.1252 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 109/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1390.0368 - acc: 0.7857 - sigLoss: -882.6074 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1384.7819 - val_acc: 0.7882 - val_sigLoss: -883.1253 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 110/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1390.2208 - acc: 0.7857 - sigLoss: -882.7234 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1384.7821 - val_acc: 0.7882 - val_sigLoss: -883.1255 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 111/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1390.1901 - acc: 0.7857 - sigLoss: -882.7048 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1384.7823 - val_acc: 0.7882 - val_sigLoss: -883.1256 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 112/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1390.1703 - acc: 0.7857 - sigLoss: -882.6924 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1384.7824 - val_acc: 0.7882 - val_sigLoss: -883.1257 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 113/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1390.2244 - acc: 0.7857 - sigLoss: -882.7265 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1384.7826 - val_acc: 0.7882 - val_sigLoss: -883.1258 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 114/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1390.1976 - acc: 0.7857 - sigLoss: -882.7089 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1384.7828 - val_acc: 0.7882 - val_sigLoss: -883.1259 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 115/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1390.3998 - acc: 0.7857 - sigLoss: -882.8381 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1384.7829 - val_acc: 0.7882 - val_sigLoss: -883.1260 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 116/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1390.2866 - acc: 0.7857 - sigLoss: -882.7661 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1384.7831 - val_acc: 0.7882 - val_sigLoss: -883.1261 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 117/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1390.4650 - acc: 0.7857 - sigLoss: -882.8792 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1384.7833 - val_acc: 0.7882 - val_sigLoss: -883.1262 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 118/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1390.2954 - acc: 0.7857 - sigLoss: -882.7723 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1384.7834 - val_acc: 0.7882 - val_sigLoss: -883.1263 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 119/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1390.4662 - acc: 0.7857 - sigLoss: -882.8798 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1384.7835 - val_acc: 0.7882 - val_sigLoss: -883.1264 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 120/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1390.3298 - acc: 0.7857 - sigLoss: -882.7927 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1384.7835 - val_acc: 0.7882 - val_sigLoss: -883.1264 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 121/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57059/57059 [==============================] - 0s 2us/step - loss: -1390.4286 - acc: 0.7857 - sigLoss: -882.8561 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1384.7836 - val_acc: 0.7882 - val_sigLoss: -883.1265 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 122/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1390.3816 - acc: 0.7857 - sigLoss: -882.8260 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1384.7837 - val_acc: 0.7882 - val_sigLoss: -883.1265 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 123/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1390.5132 - acc: 0.7857 - sigLoss: -882.9092 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1384.7838 - val_acc: 0.7882 - val_sigLoss: -883.1266 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 124/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1390.4366 - acc: 0.7857 - sigLoss: -882.8615 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1384.7840 - val_acc: 0.7882 - val_sigLoss: -883.1267 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 125/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1390.4760 - acc: 0.7857 - sigLoss: -882.8872 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1384.7840 - val_acc: 0.7882 - val_sigLoss: -883.1267 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 126/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1390.4277 - acc: 0.7857 - sigLoss: -882.8570 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1384.7840 - val_acc: 0.7882 - val_sigLoss: -883.1267 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 127/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1390.4869 - acc: 0.7857 - sigLoss: -882.8934 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1384.7841 - val_acc: 0.7882 - val_sigLoss: -883.1267 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 128/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1390.4978 - acc: 0.7857 - sigLoss: -882.9000 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1384.7841 - val_acc: 0.7882 - val_sigLoss: -883.1268 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 129/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1390.5319 - acc: 0.7857 - sigLoss: -882.9214 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1384.7842 - val_acc: 0.7882 - val_sigLoss: -883.1268 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 130/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1390.5072 - acc: 0.7857 - sigLoss: -882.9065 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1384.7842 - val_acc: 0.7882 - val_sigLoss: -883.1268 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 131/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1390.5446 - acc: 0.7857 - sigLoss: -882.9298 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1384.7843 - val_acc: 0.7882 - val_sigLoss: -883.1269 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 132/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1390.5616 - acc: 0.7857 - sigLoss: -882.9412 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1384.7843 - val_acc: 0.7882 - val_sigLoss: -883.1269 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 133/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1390.5414 - acc: 0.7857 - sigLoss: -882.9281 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1384.7844 - val_acc: 0.7882 - val_sigLoss: -883.1270 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 134/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1390.5375 - acc: 0.7857 - sigLoss: -882.9253 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1384.7844 - val_acc: 0.7882 - val_sigLoss: -883.1270 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 135/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1390.5810 - acc: 0.7857 - sigLoss: -882.9538 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1384.7845 - val_acc: 0.7882 - val_sigLoss: -883.1270 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 136/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1390.6324 - acc: 0.7857 - sigLoss: -882.9851 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1384.7845 - val_acc: 0.7882 - val_sigLoss: -883.1271 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 137/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1390.5988 - acc: 0.7857 - sigLoss: -882.9635 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1384.7846 - val_acc: 0.7882 - val_sigLoss: -883.1271 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 138/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1390.6096 - acc: 0.7857 - sigLoss: -882.9706 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1384.7846 - val_acc: 0.7882 - val_sigLoss: -883.1271 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 139/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1390.6619 - acc: 0.7857 - sigLoss: -883.0042 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1384.7846 - val_acc: 0.7882 - val_sigLoss: -883.1271 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 140/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1390.6111 - acc: 0.7857 - sigLoss: -882.9706 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1384.7847 - val_acc: 0.7882 - val_sigLoss: -883.1271 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 141/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57059/57059 [==============================] - 0s 2us/step - loss: -1390.6444 - acc: 0.7857 - sigLoss: -882.9936 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1384.7847 - val_acc: 0.7882 - val_sigLoss: -883.1271 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 142/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1390.6440 - acc: 0.7857 - sigLoss: -882.9924 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1384.7847 - val_acc: 0.7882 - val_sigLoss: -883.1272 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 143/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1390.6699 - acc: 0.7857 - sigLoss: -883.0094 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1384.7847 - val_acc: 0.7882 - val_sigLoss: -883.1272 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 144/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1390.6454 - acc: 0.7857 - sigLoss: -882.9942 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1384.7847 - val_acc: 0.7882 - val_sigLoss: -883.1272 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 145/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1390.6360 - acc: 0.7857 - sigLoss: -882.9881 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1384.7847 - val_acc: 0.7882 - val_sigLoss: -883.1272 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 146/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1390.6888 - acc: 0.7857 - sigLoss: -883.0213 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1384.7847 - val_acc: 0.7882 - val_sigLoss: -883.1272 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 147/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1390.6626 - acc: 0.7857 - sigLoss: -883.0050 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1384.7848 - val_acc: 0.7882 - val_sigLoss: -883.1272 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 148/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1390.7384 - acc: 0.7857 - sigLoss: -883.0527 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1384.7848 - val_acc: 0.7882 - val_sigLoss: -883.1272 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 149/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1390.6821 - acc: 0.7857 - sigLoss: -883.0174 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1384.7848 - val_acc: 0.7882 - val_sigLoss: -883.1272 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 150/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1390.6640 - acc: 0.7857 - sigLoss: -883.0057 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1384.7848 - val_acc: 0.7882 - val_sigLoss: -883.1272 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 151/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1390.6767 - acc: 0.7857 - sigLoss: -883.0138 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1384.7848 - val_acc: 0.7882 - val_sigLoss: -883.1272 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 152/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1390.7008 - acc: 0.7857 - sigLoss: -883.0293 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1384.7849 - val_acc: 0.7882 - val_sigLoss: -883.1273 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 153/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1390.7204 - acc: 0.7857 - sigLoss: -883.0410 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1384.7849 - val_acc: 0.7882 - val_sigLoss: -883.1273 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 154/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1390.6738 - acc: 0.7857 - sigLoss: -883.0117 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1384.7849 - val_acc: 0.7882 - val_sigLoss: -883.1273 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 155/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1390.6855 - acc: 0.7857 - sigLoss: -883.0190 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1384.7849 - val_acc: 0.7882 - val_sigLoss: -883.1273 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 156/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1390.7237 - acc: 0.7857 - sigLoss: -883.0430 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1384.7849 - val_acc: 0.7882 - val_sigLoss: -883.1273 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 157/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1390.7335 - acc: 0.7857 - sigLoss: -883.0496 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1384.7849 - val_acc: 0.7882 - val_sigLoss: -883.1273 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 158/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1390.7089 - acc: 0.7857 - sigLoss: -883.0339 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1384.7849 - val_acc: 0.7882 - val_sigLoss: -883.1273 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 159/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1390.7351 - acc: 0.7857 - sigLoss: -883.0502 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1384.7849 - val_acc: 0.7882 - val_sigLoss: -883.1273 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 160/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1390.7346 - acc: 0.7857 - sigLoss: -883.0502 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1384.7849 - val_acc: 0.7882 - val_sigLoss: -883.1273 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 161/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57059/57059 [==============================] - 0s 2us/step - loss: -1390.7664 - acc: 0.7857 - sigLoss: -883.0706 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1384.7849 - val_acc: 0.7882 - val_sigLoss: -883.1273 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 162/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1390.7645 - acc: 0.7857 - sigLoss: -883.0700 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1384.7849 - val_acc: 0.7882 - val_sigLoss: -883.1273 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 163/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1390.7326 - acc: 0.7857 - sigLoss: -883.0493 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1384.7849 - val_acc: 0.7882 - val_sigLoss: -883.1273 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 164/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1390.7429 - acc: 0.7857 - sigLoss: -883.0556 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1384.7849 - val_acc: 0.7882 - val_sigLoss: -883.1273 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 165/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1390.7796 - acc: 0.7857 - sigLoss: -883.0789 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1384.7849 - val_acc: 0.7882 - val_sigLoss: -883.1273 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 166/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1390.7809 - acc: 0.7857 - sigLoss: -883.0800 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1384.7849 - val_acc: 0.7882 - val_sigLoss: -883.1273 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 167/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1390.7247 - acc: 0.7857 - sigLoss: -883.0441 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1384.7849 - val_acc: 0.7882 - val_sigLoss: -883.1273 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 168/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1390.7721 - acc: 0.7857 - sigLoss: -883.0741 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1384.7849 - val_acc: 0.7882 - val_sigLoss: -883.1273 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 169/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1390.7762 - acc: 0.7857 - sigLoss: -883.0766 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1384.7849 - val_acc: 0.7882 - val_sigLoss: -883.1273 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 170/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1390.7758 - acc: 0.7857 - sigLoss: -883.0765 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1384.7849 - val_acc: 0.7882 - val_sigLoss: -883.1273 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 171/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1390.7851 - acc: 0.7857 - sigLoss: -883.0827 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1384.7849 - val_acc: 0.7882 - val_sigLoss: -883.1273 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 172/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1390.7610 - acc: 0.7857 - sigLoss: -883.0674 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1384.7849 - val_acc: 0.7882 - val_sigLoss: -883.1273 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 173/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1390.7678 - acc: 0.7857 - sigLoss: -883.0714 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1384.7849 - val_acc: 0.7882 - val_sigLoss: -883.1273 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 174/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1390.7736 - acc: 0.7857 - sigLoss: -883.0754 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1384.7849 - val_acc: 0.7882 - val_sigLoss: -883.1273 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 175/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1390.7599 - acc: 0.7857 - sigLoss: -883.0667 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1384.7849 - val_acc: 0.7882 - val_sigLoss: -883.1273 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 176/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1390.8033 - acc: 0.7857 - sigLoss: -883.0943 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1384.7849 - val_acc: 0.7882 - val_sigLoss: -883.1273 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 177/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1390.7871 - acc: 0.7857 - sigLoss: -883.0834 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1384.7849 - val_acc: 0.7882 - val_sigLoss: -883.1273 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 178/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1390.7946 - acc: 0.7857 - sigLoss: -883.0886 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1384.7849 - val_acc: 0.7882 - val_sigLoss: -883.1273 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 179/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1390.8053 - acc: 0.7857 - sigLoss: -883.0955 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1384.7849 - val_acc: 0.7882 - val_sigLoss: -883.1273 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 180/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1390.7969 - acc: 0.7857 - sigLoss: -883.0899 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1384.7849 - val_acc: 0.7882 - val_sigLoss: -883.1273 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 181/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57059/57059 [==============================] - 0s 2us/step - loss: -1390.8115 - acc: 0.7857 - sigLoss: -883.0994 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1384.7849 - val_acc: 0.7882 - val_sigLoss: -883.1273 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 182/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1390.8108 - acc: 0.7857 - sigLoss: -883.0990 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1384.7849 - val_acc: 0.7882 - val_sigLoss: -883.1273 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 183/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1390.8112 - acc: 0.7857 - sigLoss: -883.0991 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1384.7849 - val_acc: 0.7882 - val_sigLoss: -883.1273 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 184/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1390.8230 - acc: 0.7857 - sigLoss: -883.1066 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1384.7849 - val_acc: 0.7882 - val_sigLoss: -883.1273 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 185/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1390.7864 - acc: 0.7857 - sigLoss: -883.0832 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1384.7849 - val_acc: 0.7882 - val_sigLoss: -883.1273 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 186/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1390.7949 - acc: 0.7857 - sigLoss: -883.0884 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1384.7849 - val_acc: 0.7882 - val_sigLoss: -883.1273 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 187/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1390.8140 - acc: 0.7857 - sigLoss: -883.1007 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1384.7849 - val_acc: 0.7882 - val_sigLoss: -883.1273 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 188/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1390.8069 - acc: 0.7857 - sigLoss: -883.0962 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1384.7849 - val_acc: 0.7882 - val_sigLoss: -883.1273 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 189/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1390.8012 - acc: 0.7857 - sigLoss: -883.0928 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1384.7849 - val_acc: 0.7882 - val_sigLoss: -883.1273 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 190/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1390.8262 - acc: 0.7857 - sigLoss: -883.1086 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1384.7849 - val_acc: 0.7882 - val_sigLoss: -883.1273 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 191/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1390.8197 - acc: 0.7857 - sigLoss: -883.1045 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1384.7849 - val_acc: 0.7882 - val_sigLoss: -883.1273 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 192/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1390.8102 - acc: 0.7857 - sigLoss: -883.0983 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1384.7849 - val_acc: 0.7882 - val_sigLoss: -883.1273 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 193/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1390.8003 - acc: 0.7857 - sigLoss: -883.0921 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1384.7849 - val_acc: 0.7882 - val_sigLoss: -883.1273 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 194/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1390.7826 - acc: 0.7857 - sigLoss: -883.0808 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1384.7849 - val_acc: 0.7882 - val_sigLoss: -883.1273 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 195/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1390.8081 - acc: 0.7857 - sigLoss: -883.0970 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1384.7849 - val_acc: 0.7882 - val_sigLoss: -883.1273 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 196/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1390.8128 - acc: 0.7857 - sigLoss: -883.1000 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1384.7849 - val_acc: 0.7882 - val_sigLoss: -883.1273 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 197/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1390.7903 - acc: 0.7857 - sigLoss: -883.0856 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1384.7849 - val_acc: 0.7882 - val_sigLoss: -883.1273 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 198/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1390.8000 - acc: 0.7857 - sigLoss: -883.0916 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1384.7849 - val_acc: 0.7882 - val_sigLoss: -883.1273 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 199/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1390.8072 - acc: 0.7857 - sigLoss: -883.0964 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1384.7849 - val_acc: 0.7882 - val_sigLoss: -883.1273 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n",
      "Epoch 200/200\n",
      "57059/57059 [==============================] - 0s 2us/step - loss: -1390.8392 - acc: 0.7857 - sigLoss: -883.1167 - significance: 29.7175 - asimovSignificance: 9.0931 - truePositive: 1.0000 - falsePositive: 1.0000 - val_loss: -1384.7849 - val_acc: 0.7882 - val_sigLoss: -883.1273 - val_significance: 29.7175 - val_asimovSignificance: 9.0931 - val_truePositive: 1.0000 - val_falsePositive: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1024/24455 [>.............................] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "24455/24455 [==============================] - 0s 1us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/felipe/hepML/MlClasses/Dnn.py:221: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  report = self.model.evaluate(X_test.as_matrix(), y_test.as_matrix(), sample_weight=weights_test, batch_size=batchSize)\n",
      "/home/felipe/hepML/MlClasses/Dnn.py:227: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  classificationReport(self.model.predict_classes(X_test.as_matrix()),self.model.predict(X_test.as_matrix()),y_test,f)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57059/57059 [==============================] - 0s 1us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/felipe/anaconda3/envs/tf-gpu/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/felipe/hepML/MlClasses/Dnn.py:232: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  report = self.model.evaluate(X_train.as_matrix(), y_train.as_matrix(), sample_weight=weights_train, batch_size=batchSize)\n",
      "/home/felipe/hepML/MlClasses/Dnn.py:236: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  classificationReport(self.model.predict_classes(X_train.as_matrix()),self.model.predict(X_train.as_matrix()),y_train,f)\n",
      "/home/felipe/anaconda3/envs/tf-gpu/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/felipe/hepML/MlClasses/Dnn.py:254: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  rocCurve(self.model.predict(self.data.X_test.as_matrix()), self.data.y_test,self.output)\n",
      "/home/felipe/hepML/MlClasses/Dnn.py:255: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  rocCurve(self.model.predict(self.data.X_train.as_matrix()),self.data.y_train,self.output,append='_train')\n",
      "/home/felipe/hepML/MlClasses/Dnn.py:263: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  compareTrainTest(self.model.predict,self.data.X_train.as_matrix(),self.data.y_train.as_matrix(),\\\n",
      "/home/felipe/hepML/MlClasses/Dnn.py:264: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  self.data.X_test.as_matrix(),self.data.y_test.as_matrix(),self.output)\n",
      "/home/felipe/anaconda3/envs/tf-gpu/lib/python3.6/site-packages/matplotlib/axes/_axes.py:6521: MatplotlibDeprecationWarning: \n",
      "The 'normed' kwarg was deprecated in Matplotlib 2.1 and will be removed in 3.1. Use 'density' instead.\n",
      "  alternative=\"'density'\", removal=\"3.1\")\n",
      "/home/felipe/anaconda3/envs/tf-gpu/lib/python3.6/site-packages/numpy/lib/histograms.py:823: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  return n/db/n.sum(), bin_edges\n",
      "/home/felipe/anaconda3/envs/tf-gpu/lib/python3.6/site-packages/numpy/lib/histograms.py:823: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return n/db/n.sum(), bin_edges\n",
      "/home/felipe/hepML/MlClasses/PerformanceTests.py:79: VisibleDeprecationWarning: Passing `normed=True` on non-uniform bins has always been broken, and computes neither the probability density function nor the probability mass function. The result is only correct if the bins are uniform, when density=True will produce the same result anyway. The argument will be removed in a future version of numpy.\n",
      "  bins=bins, range=low_high, normed=True)\n",
      "/home/felipe/anaconda3/envs/tf-gpu/lib/python3.6/site-packages/numpy/lib/histograms.py:838: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  return n/(n*db).sum(), bin_edges\n",
      "/home/felipe/anaconda3/envs/tf-gpu/lib/python3.6/site-packages/numpy/lib/histograms.py:838: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return n/(n*db).sum(), bin_edges\n",
      "/home/felipe/hepML/MlClasses/PerformanceTests.py:88: VisibleDeprecationWarning: Passing `normed=True` on non-uniform bins has always been broken, and computes neither the probability density function nor the probability mass function. The result is only correct if the bins are uniform, when density=True will produce the same result anyway. The argument will be removed in a future version of numpy.\n",
      "  bins=bins, range=low_high, normed=True)\n",
      "/home/felipe/anaconda3/envs/tf-gpu/lib/python3.6/site-packages/ipykernel_launcher.py:77: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "/home/felipe/hepML/MlClasses/Dnn.py:405: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  dataTest['truth']=self.data.y_test.as_matrix()\n"
     ]
    }
   ],
   "source": [
    "# I'm only running the DNN with binary cross entropy, letter I will include the other options.\n",
    "# so far it running perfect, I'm testing with their custom loss function.\n",
    "for varSetName,varSet in chosenVars.items():\n",
    "    #Pick out the expanded arrays\n",
    "    columnsInDataFrame = []\n",
    "    for k in combined.keys():\n",
    "        for v in varSet:\n",
    "            #Little trick to ensure only the start of the string is checked\n",
    "            if varSetName is '0L':\n",
    "                if ' '+v+' ' in ' '+k+' ': columnsInDataFrame.append(k)\n",
    "            elif ' '+v in ' '+k: columnsInDataFrame.append(k)\n",
    "\n",
    "\n",
    "    #Select just the features we're interested in\n",
    "    #For now setting NaNs to 0 for compatibility\n",
    "    combinedToRun = combined[columnsInDataFrame].copy()\n",
    "    combinedToRun.fillna(0,inplace=True)\n",
    "    \n",
    "    combinedToRun.index = np.arange(0,200000)\n",
    "    mlData = MlData(combinedToRun,'signal')\n",
    "\n",
    "    mlData.prepare(evalSize=0.0,testSize=0.3,limitSize=None)\n",
    "\n",
    "    for name,config in dnnConfigs.items():\n",
    "        dnn = Dnn(mlData,'testPlots/mlPlots/crossEntropyFirst/bslike/'+varSetName+'/'+name)\n",
    "        dnn.setup(hiddenLayers=config['hiddenLayers'],dropOut=config['dropOut'],\n",
    "                  l2Regularization=config['l2Regularization'], optimizer=config['optimizer'],\n",
    "                  activation=config['activation'],\n",
    "                extraMetrics=[\n",
    "                    significanceLoss(expectedSignal,expectedBkgd),significanceFull(expectedSignal,expectedBkgd),\n",
    "                    asimovSignificanceFull(expectedSignal,expectedBkgd,systematic),truePositive,falsePositive\n",
    "                    ])\n",
    "        dnn.fit(epochs=config['epochs'],batch_size=config['batch_size'])\n",
    "        dnn.diagnostics(batchSize=128,subDir='pretraining')\n",
    "        dnn.makeHepPlots(expectedSignal,expectedBkgd,asimovSigLossSysts,makeHistograms=False,subDir='pretraining')\n",
    "        \n",
    "        #Now cut away the obvious background, weight up the background events and retrain\n",
    "\n",
    "        #Remove all x entropy background\n",
    "        dataToPredict = combinedToRun.drop('signal',axis=1)\n",
    "        dataToPredict = dataToPredict.loc[:,~dataToPredict.columns.duplicated()]\n",
    "        dataToPredict = dnn.data.scaler.transform(dataToPredict.as_matrix())\n",
    "        toRunXEntropyFirst = combinedToRun[dnn.model.predict(dataToPredict)>0.5]\n",
    "\n",
    "        #weight up the background based on the inbalance\n",
    "        nSignal = len(toRunXEntropyFirst[toRunXEntropyFirst.signal==1])\n",
    "        nBkgd = len(toRunXEntropyFirst[toRunXEntropyFirst.signal==0])\n",
    "        upWeight = float(nSignal)/nBkgd\n",
    "\n",
    "        def fabricateWeight(signal,weight):\n",
    "            if signal==0: return weight\n",
    "            else: return 1.0\n",
    "\n",
    "        weights = toRunXEntropyFirst.apply(lambda row: fabricateWeight(row.signal,upWeight),axis=1) \n",
    "\n",
    "        mlDataXEntropyFirst = MlData(toRunXEntropyFirst,'signal',weights=weights.as_matrix())\n",
    "        mlDataXEntropyFirst.prepare(evalSize=0.0,testSize=0.3,limitSize=None)\n",
    "\n",
    "        dnn2 = Dnn(mlDataXEntropyFirst,'testPlots/mlPlots/crossEntropyFirst/'+varSetName+'/'+name)\n",
    "        dnn2.setup(hiddenLayers=config['hiddenLayers'],dropOut=config['dropOut'],\n",
    "                   l2Regularization=config['l2Regularization'], optimizer=config['optimizer'],\n",
    "                  activation=config['activation'],\n",
    "                loss=significanceLoss(expectedSignal,expectedBkgd),\n",
    "                extraMetrics=[\n",
    "                    significanceLoss(expectedSignal,expectedBkgd),significanceFull(expectedSignal,expectedBkgd),\n",
    "                    asimovSignificanceFull(expectedSignal,expectedBkgd,systematic),truePositive,falsePositive\n",
    "                ])\n",
    "        dnn2.fit(epochs=config['epochs'],batch_size=config['batch_size'])\n",
    "        dnn2.diagnostics(batchSize=config['batch_size'])\n",
    "\n",
    "        #Now need to make the HEP plots with the full dataset by passing a custom prediction that includes the decision from dnn2\n",
    "        # In this case we take xEntropy Score > 0.5 and then the sig loss output\n",
    "        # i.e. if xEntropyScore < 0.5 return 0, else return sig loss output\n",
    "\n",
    "        #save the prediction from dnn\n",
    "        #firstPred = dnn.testPrediction()\n",
    "        firstPred = dnn.model.predict_classes(dnn.data.X_test.as_matrix())\n",
    "        #unscale the data in dnn\n",
    "        dataForPred2=dnn.data.scaler.inverse_transform(dnn.data.X_test)\n",
    "        #rescale the data to dnn2\n",
    "        dataForPred2=dnn2.data.scaler.transform(dataForPred2)\n",
    "        #predict dnn2\n",
    "        secondPred = dnn2.model.predict(dataForPred2)\n",
    "        #combine the predictions\n",
    "        finalPred = firstPred*secondPred\n",
    "\n",
    "        dnn.makeHepPlots(expectedSignal,expectedBkgd,asimovSigLossSysts,makeHistograms=False,customPrediction=finalPred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow-GPU",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
