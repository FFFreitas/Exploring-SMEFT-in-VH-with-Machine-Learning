{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First load os and sys so I can update the sys.path with new functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "change the luminosity to 80 /fb\n",
    "\n",
    "generate the 3 plots as in aewol paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#take the paths to the functions we nedd\n",
    "module_path = os.path.abspath(os.path.join('./pandasPlotting/'))\n",
    "module2_path = os.path.abspath(os.path.join('./MlClasses/'))\n",
    "module3_path = os.path.abspath(os.path.join('./MlFunctions/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this part will include in the sys.path variables the paths for our new functions\n",
    "if [module_path, module2_path, module3_path] not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# here we are going to load what we will need, keras + tensorflow, plot functions, etc..\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "\n",
    "from keras import callbacks\n",
    "\n",
    "from pandasPlotting.Plotter import Plotter\n",
    "from pandasPlotting.dfFunctions import expandArrays\n",
    "from pandasPlotting.dtFunctions import featureImportance\n",
    "\n",
    "from MlClasses.MlData import MlData\n",
    "from MlClasses.Bdt import Bdt\n",
    "from MlClasses.Dnn import Dnn\n",
    "from MlClasses.ComparePerformances import ComparePerformances\n",
    "\n",
    "from MlFunctions.DnnFunctions import significanceLoss,significanceLossInvert,significanceLoss2Invert ,significanceLossInvertSqrt,significanceFull,asimovSignificanceLoss,asimovSignificanceLossInvert,asimovSignificanceFull,truePositive,falsePositive\n",
    "\n",
    "from linearAlgebraFunctions import gram,addGramToFlatDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I don't have patience to training 200 epochs ¯\\_(ツ)_/¯\n",
    "earlyStopping = callbacks.EarlyStopping(monitor='val_loss',min_delta=0,patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load our data files\n",
    "signal=pd.read_csv(\"../pyROOT_CPV_CPC/pp_wh/2dplots/analysis_with_cut/cpv_scan/charanjit_data/0L/feb/feb-0L-inclusive/zh0p030lfeb.csv\",sep='\\s+',engine='python')\n",
    "bkgd=pd.read_csv(\"../pyROOT_CPV_CPC/pp_wh/2dplots/analysis_with_cut/cpv_scan/charanjit_data/0L/feb/feb-0L-inclusive/zhsm0lfeb.csv\",sep='\\s+',engine='python')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine them into one dataset\n",
    "combined = pd.concat([signal,bkgd]).sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['ptb1', 'ptb2', 'misset', 'pth', 'ptz', 'etah', 'phih', 'mtvh', 'ptvh',\n",
      "       'dphib1met', 'signal'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(combined.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change thes vars depend on which dataset you are loading, I will implement a better solution.\n",
    "chosenVars = {\n",
    "            # #A vanilla analysis with HL variables and lead 3 jets\n",
    "            '0L':['ptb1', 'ptb2', 'misset', 'pth', 'ptz', 'etah', 'phih', 'mtvh', 'ptvh', 'dphib1met', 'signal'],\n",
    "            #'2L':['ptb1', 'ptb2', 'ptl1', 'ptl2', 'pth', 'ptz', 'etah', 'phih',\n",
    "            #     'deltarll', 'deltarbl', 'mtvh', 'ptvh', 'dphil1b1', 'dphil1b2', 'signal']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainedModels={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#needed to plot asimov significane\n",
    "asimovSigLossSysts=[0.01,0.05,0.1,0.2,0.3,0.4,0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here I have included one archtecture I got from my ES scan, pls comment my entry and use dnn_batch4096 instead.\n",
    "dnnConfigs={\n",
    "    #'dnn_ZH_0L_cHW_0d001_batch_4096':{'epochs':200,'batch_size':4096,'dropOut':None,\n",
    "    #                                  'l2Regularization':None,'hiddenLayers':[1.0],\n",
    "    #                                  'optimizer':'adam', 'activation':'relu'}\n",
    "    #'dnn_ZH_0L_cHW_0d01_batch_4096':{'epochs':200,'batch_size':4096,'dropOut':None,\n",
    "    #                                  'l2Regularization':None,'hiddenLayers':[1.0],\n",
    "    #                                  'optimizer':'adam', 'activation':'relu'}\n",
    "    #'dnn_ZH_0L_cHW_0d03_batch_4096':{'epochs':5,'batch_size':4096,'dropOut':None,\n",
    "    #                                 'l2Regularization':None,'hiddenLayers':[1.0],\n",
    "    #                                 'optimizer':'adam', 'activation':'relu'}\n",
    "    #'dnn_ZH_0L_cHW_0d03_batch_8192':{'epochs':20,'batch_size':8192,'dropOut':None,\n",
    "    #                                  'l2Regularization':None,'hiddenLayers':[1.0],\n",
    "    #                                  'optimizer':'adam', 'activation':'relu'}    \n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bsmlike signal cHW 0.03: 11.867fb, bg:0.89\n",
    "#inclusive signal cHW 0.001: 2.198fb, bg:2.03fb\n",
    "#inclusive signal cHW 0.01: 4.553fb, bg:2.03fb\n",
    "#inclusive signal cHW 0.03: 14fb, bg:2.03fb\n",
    "\n",
    "lumi=80. #luminosity in /fb\n",
    "expectedSignal=14*lumi \n",
    "expectedBkgd=2.03*lumi #cross section of ttbar sample in fb times efficiency measured by Marco\n",
    "systematic=0.5 #systematic for the asimov signficance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/felipe/hepML/MlClasses/Dnn.py:101: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  self.history = self.model.fit(self.data.X_train.as_matrix(), self.data.y_train.as_matrix(), sample_weight=self.data.weights_train,\n",
      "/home/felipe/hepML/MlClasses/Dnn.py:102: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  validation_data=(self.data.X_test.as_matrix(),self.data.y_test.as_matrix(),self.data.weights_test),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 140000 samples, validate on 60000 samples\n",
      "Epoch 1/20\n",
      "140000/140000 [==============================] - 1s 5us/step - loss: 0.0019 - acc: 0.5637 - sigLoss: -531.9808 - significance: 24.8096 - asimovSignificance: 7.0572 - truePositive: 0.6143 - falsePositive: 0.4870 - val_loss: 0.0018 - val_acc: 0.5743 - val_sigLoss: -563.2481 - val_significance: 26.7857 - val_asimovSignificance: 7.0786 - val_truePositive: 0.7143 - val_falsePositive: 0.5663\n",
      "Epoch 2/20\n",
      "140000/140000 [==============================] - 0s 1us/step - loss: 0.0017 - acc: 0.5685 - sigLoss: -584.3498 - significance: 27.5950 - asimovSignificance: 6.9484 - truePositive: 0.7609 - falsePositive: 0.6234 - val_loss: 0.0016 - val_acc: 0.5658 - val_sigLoss: -608.3345 - val_significance: 28.4643 - val_asimovSignificance: 6.8504 - val_truePositive: 0.8115 - val_falsePositive: 0.6812\n",
      "Epoch 3/20\n",
      "140000/140000 [==============================] - 0s 1us/step - loss: 0.0016 - acc: 0.5556 - sigLoss: -624.6038 - significance: 28.8138 - asimovSignificance: 6.7262 - truePositive: 0.8345 - falsePositive: 0.7226 - val_loss: 0.0016 - val_acc: 0.5534 - val_sigLoss: -643.2122 - val_significance: 29.3089 - val_asimovSignificance: 6.6694 - val_truePositive: 0.8647 - val_falsePositive: 0.7596\n",
      "Epoch 4/20\n",
      "140000/140000 [==============================] - 0s 1us/step - loss: 0.0015 - acc: 0.5448 - sigLoss: -656.3480 - significance: 29.5196 - asimovSignificance: 6.5861 - truePositive: 0.8793 - falsePositive: 0.7889 - val_loss: 0.0015 - val_acc: 0.5435 - val_sigLoss: -671.7570 - val_significance: 29.8517 - val_asimovSignificance: 6.5511 - val_truePositive: 0.9001 - val_falsePositive: 0.8149\n",
      "Epoch 5/20\n",
      "140000/140000 [==============================] - 0s 1us/step - loss: 0.0015 - acc: 0.5369 - sigLoss: -683.0249 - significance: 29.9930 - asimovSignificance: 6.4967 - truePositive: 0.9101 - falsePositive: 0.8355 - val_loss: 0.0014 - val_acc: 0.5361 - val_sigLoss: -696.3594 - val_significance: 30.2420 - val_asimovSignificance: 6.4706 - val_truePositive: 0.9260 - val_falsePositive: 0.8559\n",
      "Epoch 6/20\n",
      "140000/140000 [==============================] - 0s 1us/step - loss: 0.0014 - acc: 0.5307 - sigLoss: -706.2656 - significance: 30.3347 - asimovSignificance: 6.4327 - truePositive: 0.9328 - falsePositive: 0.8706 - val_loss: 0.0014 - val_acc: 0.5300 - val_sigLoss: -718.0010 - val_significance: 30.5501 - val_asimovSignificance: 6.4103 - val_truePositive: 0.9467 - val_falsePositive: 0.8888\n",
      "Epoch 7/20\n",
      "140000/140000 [==============================] - 0s 1us/step - loss: 0.0014 - acc: 0.5251 - sigLoss: -726.8570 - significance: 30.6148 - asimovSignificance: 6.3793 - truePositive: 0.9517 - falsePositive: 0.9007 - val_loss: 0.0014 - val_acc: 0.5249 - val_sigLoss: -737.3499 - val_significance: 30.7739 - val_asimovSignificance: 6.3625 - val_truePositive: 0.9621 - val_falsePositive: 0.9146\n",
      "Epoch 8/20\n",
      "140000/140000 [==============================] - 0s 1us/step - loss: 0.0013 - acc: 0.5191 - sigLoss: -745.3518 - significance: 30.7934 - asimovSignificance: 6.3266 - truePositive: 0.9644 - falsePositive: 0.9253 - val_loss: 0.0013 - val_acc: 0.5190 - val_sigLoss: -754.8543 - val_significance: 30.9222 - val_asimovSignificance: 6.3114 - val_truePositive: 0.9730 - val_falsePositive: 0.9373\n",
      "Epoch 9/20\n",
      "140000/140000 [==============================] - 0s 1us/step - loss: 0.0013 - acc: 0.5145 - sigLoss: -762.1501 - significance: 30.9449 - asimovSignificance: 6.2877 - truePositive: 0.9752 - falsePositive: 0.9452 - val_loss: 0.0013 - val_acc: 0.5152 - val_sigLoss: -770.7762 - val_significance: 31.0438 - val_asimovSignificance: 6.2796 - val_truePositive: 0.9817 - val_falsePositive: 0.9535\n",
      "Epoch 10/20\n",
      "140000/140000 [==============================] - 0s 1us/step - loss: 0.0013 - acc: 0.5107 - sigLoss: -777.4411 - significance: 31.0452 - asimovSignificance: 6.2565 - truePositive: 0.9825 - falsePositive: 0.9601 - val_loss: 0.0013 - val_acc: 0.5113 - val_sigLoss: -785.3297 - val_significance: 31.1163 - val_asimovSignificance: 6.2475 - val_truePositive: 0.9873 - val_falsePositive: 0.9671\n",
      "Epoch 11/20\n",
      "140000/140000 [==============================] - 0s 1us/step - loss: 0.0013 - acc: 0.5075 - sigLoss: -791.4560 - significance: 31.1211 - asimovSignificance: 6.2316 - truePositive: 0.9881 - falsePositive: 0.9719 - val_loss: 0.0013 - val_acc: 0.5085 - val_sigLoss: -798.6762 - val_significance: 31.1709 - val_asimovSignificance: 6.2254 - val_truePositive: 0.9915 - val_falsePositive: 0.9769\n",
      "Epoch 12/20\n",
      "140000/140000 [==============================] - 0s 1us/step - loss: 0.0012 - acc: 0.5051 - sigLoss: -804.3495 - significance: 31.1713 - asimovSignificance: 6.2120 - truePositive: 0.9919 - falsePositive: 0.9808 - val_loss: 0.0012 - val_acc: 0.5061 - val_sigLoss: -810.9428 - val_significance: 31.1998 - val_asimovSignificance: 6.2064 - val_truePositive: 0.9939 - val_falsePositive: 0.9842\n",
      "Epoch 13/20\n",
      "140000/140000 [==============================] - 0s 1us/step - loss: 0.0012 - acc: 0.5033 - sigLoss: -816.1939 - significance: 31.2052 - asimovSignificance: 6.1985 - truePositive: 0.9945 - falsePositive: 0.9868 - val_loss: 0.0012 - val_acc: 0.5047 - val_sigLoss: -822.2993 - val_significance: 31.2228 - val_asimovSignificance: 6.1953 - val_truePositive: 0.9957 - val_falsePositive: 0.9889\n",
      "Epoch 14/20\n",
      "140000/140000 [==============================] - 0s 1us/step - loss: 0.0012 - acc: 0.5024 - sigLoss: -827.1495 - significance: 31.2370 - asimovSignificance: 6.1917 - truePositive: 0.9968 - falsePositive: 0.9909 - val_loss: 0.0012 - val_acc: 0.5036 - val_sigLoss: -832.7867 - val_significance: 31.2440 - val_asimovSignificance: 6.1871 - val_truePositive: 0.9974 - val_falsePositive: 0.9926\n",
      "Epoch 15/20\n",
      "140000/140000 [==============================] - 0s 1us/step - loss: 0.0012 - acc: 0.5014 - sigLoss: -837.3037 - significance: 31.2509 - asimovSignificance: 6.1841 - truePositive: 0.9979 - falsePositive: 0.9940 - val_loss: 0.0012 - val_acc: 0.5029 - val_sigLoss: -842.4873 - val_significance: 31.2546 - val_asimovSignificance: 6.1815 - val_truePositive: 0.9982 - val_falsePositive: 0.9950\n",
      "Epoch 16/20\n",
      "140000/140000 [==============================] - 0s 1us/step - loss: 0.0012 - acc: 0.5006 - sigLoss: -846.7018 - significance: 31.2602 - asimovSignificance: 6.1780 - truePositive: 0.9987 - falsePositive: 0.9964 - val_loss: 0.0012 - val_acc: 0.5023 - val_sigLoss: -851.5339 - val_significance: 31.2639 - val_asimovSignificance: 6.1774 - val_truePositive: 0.9990 - val_falsePositive: 0.9968\n",
      "Epoch 17/20\n",
      "140000/140000 [==============================] - 0s 1us/step - loss: 0.0012 - acc: 0.5003 - sigLoss: -855.4706 - significance: 31.2685 - asimovSignificance: 6.1752 - truePositive: 0.9993 - falsePositive: 0.9977 - val_loss: 0.0012 - val_acc: 0.5021 - val_sigLoss: -859.9530 - val_significance: 31.2697 - val_asimovSignificance: 6.1756 - val_truePositive: 0.9994 - val_falsePositive: 0.9977\n",
      "Epoch 18/20\n",
      "140000/140000 [==============================] - 0s 1us/step - loss: 0.0012 - acc: 0.4998 - sigLoss: -863.6547 - significance: 31.2718 - asimovSignificance: 6.1720 - truePositive: 0.9997 - falsePositive: 0.9989 - val_loss: 0.0012 - val_acc: 0.5016 - val_sigLoss: -867.8330 - val_significance: 31.2724 - val_asimovSignificance: 6.1718 - val_truePositive: 0.9997 - val_falsePositive: 0.9990\n",
      "Epoch 19/20\n",
      "140000/140000 [==============================] - 0s 1us/step - loss: 0.0011 - acc: 0.4997 - sigLoss: -871.3079 - significance: 31.2737 - asimovSignificance: 6.1706 - truePositive: 0.9998 - falsePositive: 0.9994 - val_loss: 0.0011 - val_acc: 0.5015 - val_sigLoss: -875.1949 - val_significance: 31.2738 - val_asimovSignificance: 6.1705 - val_truePositive: 0.9998 - val_falsePositive: 0.9995\n",
      "Epoch 20/20\n",
      "140000/140000 [==============================] - 0s 1us/step - loss: 0.0011 - acc: 0.4996 - sigLoss: -878.4524 - significance: 31.2750 - asimovSignificance: 6.1701 - truePositive: 0.9999 - falsePositive: 0.9997 - val_loss: 0.0011 - val_acc: 0.5014 - val_sigLoss: -882.0820 - val_significance: 31.2745 - val_asimovSignificance: 6.1699 - val_truePositive: 0.9999 - val_falsePositive: 0.9997\n",
      "60000/60000 [==============================] - 0s 0us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/felipe/hepML/MlClasses/Dnn.py:221: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  report = self.model.evaluate(X_test.as_matrix(), y_test.as_matrix(), sample_weight=weights_test, batch_size=batchSize)\n",
      "/home/felipe/hepML/MlClasses/Dnn.py:227: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  classificationReport(self.model.predict_classes(X_test.as_matrix()),self.model.predict(X_test.as_matrix()),y_test,f)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140000/140000 [==============================] - 0s 0us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/felipe/hepML/MlClasses/Dnn.py:232: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  report = self.model.evaluate(X_train.as_matrix(), y_train.as_matrix(), sample_weight=weights_train, batch_size=batchSize)\n",
      "/home/felipe/hepML/MlClasses/Dnn.py:236: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  classificationReport(self.model.predict_classes(X_train.as_matrix()),self.model.predict(X_train.as_matrix()),y_train,f)\n",
      "/home/felipe/hepML/MlClasses/Dnn.py:254: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  rocCurve(self.model.predict(self.data.X_test.as_matrix()), self.data.y_test,self.output)\n",
      "/home/felipe/hepML/MlClasses/Dnn.py:255: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  rocCurve(self.model.predict(self.data.X_train.as_matrix()),self.data.y_train,self.output,append='_train')\n",
      "/home/felipe/hepML/MlClasses/Dnn.py:263: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  compareTrainTest(self.model.predict,self.data.X_train.as_matrix(),self.data.y_train.as_matrix(),\\\n",
      "/home/felipe/hepML/MlClasses/Dnn.py:264: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  self.data.X_test.as_matrix(),self.data.y_test.as_matrix(),self.output)\n",
      "/home/felipe/anaconda3/envs/tf-gpu/lib/python3.6/site-packages/matplotlib/axes/_axes.py:6521: MatplotlibDeprecationWarning: \n",
      "The 'normed' kwarg was deprecated in Matplotlib 2.1 and will be removed in 3.1. Use 'density' instead.\n",
      "  alternative=\"'density'\", removal=\"3.1\")\n",
      "/home/felipe/hepML/MlClasses/PerformanceTests.py:79: VisibleDeprecationWarning: Passing `normed=True` on non-uniform bins has always been broken, and computes neither the probability density function nor the probability mass function. The result is only correct if the bins are uniform, when density=True will produce the same result anyway. The argument will be removed in a future version of numpy.\n",
      "  bins=bins, range=low_high, normed=True)\n",
      "/home/felipe/hepML/MlClasses/PerformanceTests.py:88: VisibleDeprecationWarning: Passing `normed=True` on non-uniform bins has always been broken, and computes neither the probability density function nor the probability mass function. The result is only correct if the bins are uniform, when density=True will produce the same result anyway. The argument will be removed in a future version of numpy.\n",
      "  bins=bins, range=low_high, normed=True)\n",
      "/home/felipe/hepML/MlClasses/Dnn.py:368: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  return self.model.predict(self.data.X_test.as_matrix())\n",
      "/home/felipe/hepML/MlClasses/Dnn.py:405: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  dataTest['truth']=self.data.y_test.as_matrix()\n"
     ]
    }
   ],
   "source": [
    "# I'm only running the DNN with binary cross entropy, letter I will include the other options.\n",
    "# so far it running perfect, I'm testing with their custom loss function.\n",
    "for varSetName,varSet in chosenVars.items():\n",
    "    #Pick out the expanded arrays\n",
    "    columnsInDataFrame = []\n",
    "    for k in combined.keys():\n",
    "        for v in varSet:\n",
    "            #Little trick to ensure only the start of the string is checked\n",
    "            if varSetName is '0L':\n",
    "                if ' '+v+' ' in ' '+k+' ': columnsInDataFrame.append(k)\n",
    "            elif ' '+v in ' '+k: columnsInDataFrame.append(k)\n",
    "\n",
    "\n",
    "    #Select just the features we're interested in\n",
    "    #For now setting NaNs to 0 for compatibility\n",
    "    combinedToRun = combined[columnsInDataFrame].copy()\n",
    "    combinedToRun.fillna(0,inplace=True)\n",
    "    \n",
    "    combinedToRun.index = np.arange(0,200000)\n",
    "    mlData = MlData(combinedToRun,'signal')\n",
    "\n",
    "    mlData.prepare(evalSize=0.0,testSize=0.3,limitSize=None)\n",
    "\n",
    "    for name,config in dnnConfigs.items():\n",
    "        dnn = Dnn(mlData,'testPlots/mlPlots/sigLossInvert/inclusive/'+varSetName+'/'+name)\n",
    "        dnn.setup(hiddenLayers=config['hiddenLayers'],dropOut=config['dropOut'],\n",
    "                  l2Regularization=config['l2Regularization'], optimizer=config['optimizer'],\n",
    "                  activation=config['activation'],\n",
    "                loss=significanceLossInvert(expectedSignal,expectedBkgd),\n",
    "                extraMetrics=[\n",
    "                    significanceLoss(expectedSignal,expectedBkgd),significanceFull(expectedSignal,expectedBkgd),\n",
    "                    asimovSignificanceFull(expectedSignal,expectedBkgd,systematic),truePositive,falsePositive\n",
    "                ])\n",
    "        dnn.fit(epochs=config['epochs'],batch_size=config['batch_size'], callbacks=[earlyStopping])\n",
    "        dnn.diagnostics(batchSize=config['batch_size'])\n",
    "        dnn.makeHepPlots(expectedSignal,expectedBkgd,asimovSigLossSysts,makeHistograms=False)\n",
    "\n",
    "        trainedModels[varSetName+'_sigLossInvert_'+name]=dnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow-GPU",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
